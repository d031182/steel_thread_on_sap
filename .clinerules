# Development Standards & Best Practices

**Version**: 4.1  
**Last Updated**: 2026-02-06 (Feng Shui Phase 4-17: Multi-Agent Intelligence + Infinite Loop Fix)
**Purpose**: Core standards for P2P Data Products development

---

## ‚ö†Ô∏è CRITICAL: ARCHITECTURE-FIRST ENFORCEMENT

**MANDATORY RULE**: When user discusses architecture for 90+ minutes, IMPLEMENT ARCHITECTURE FIRST!

### The Problem We're Preventing
- ‚ùå User spends 90 min discussing modular architecture
- ‚ùå AI ignores discussion, implements features with inline/hardwired code
- ‚ùå Result: Double work (implement ‚Üí refactor), fragile code, technical debt
- ‚ùå User frustration: "We could have avoided this!"

### The Correct Approach
- ‚úÖ User discusses architecture for 90+ minutes ‚Üí **THAT IS THE PRIORITY**
- ‚úÖ AI asks: "Should I implement the architecture discussed before adding features?"
- ‚úÖ AI implements: Interfaces ‚Üí Modules ‚Üí Integration ‚Üí THEN features
- ‚úÖ Result: Features built on solid foundation, zero refactoring needed

### Enforcement Checklist (AI MUST DO THIS)
Before implementing ANY feature, AI must ask in `<thinking>`:
1. ‚ùì Has user discussed architecture extensively (60+ minutes)?
2. ‚ùì Are there unimplemented architectural concepts (interfaces, registries, DI)?
3. ‚ùì Am I about to hardwire code that should use the discussed architecture?
4. ‚ùì Will this create technical debt that requires later refactoring?

**If ANY answer is YES**: STOP. Ask user: "Should I implement [architecture concept] first?"

---

## üéØ PRIORITY 0: SAFETY CHECKPOINT ‚≠ê CRITICAL

### 0. Clean Git State Before Critical Changes ‚ö†Ô∏è MANDATORY

**RULE**: Before any risky/critical operation, ensure clean git state with remote backup

**What Qualifies as "Critical Changes"**:
- ‚úÖ Architecture refactoring (moving files, renaming modules)
- ‚úÖ Database schema changes (migrations, table modifications)
- ‚úÖ Core infrastructure changes (DI refactoring, interface changes)
- ‚úÖ Build system changes (package.json, requirements.txt)
- ‚úÖ Configuration changes (.gitignore, pytest.ini, feature_flags.json)
- ‚úÖ Multi-file operations (mass renames, bulk updates)

**Safety Checkpoint Workflow**:

```bash
# 1. Check git status
git status

# 2. If changes exist:
git add .
git commit -m "checkpoint: [brief description of current state]"
git push origin main

# 3. Now safe to proceed with critical operation
```

**Why This Matters**:
- Last night's catastrophe: Feng Shui migration crashed mid-way
- Without clean commit: Would have lost work + had inconsistent state
- With clean commit: Can rollback instantly via `git reset --hard HEAD~1`
- User philosophy: "Always have a safe rollback point"

**AI Enforcement** (MANDATORY):

Before ANY critical operation, AI must ask in `<thinking>`:
1. ‚ùì Is this a critical/risky operation (see list above)?
2. ‚ùì Are there uncommitted changes in git?
3. ‚ùì Do I have a clean rollback point?

**If operation is critical AND there are uncommitted changes**: 
- STOP and ask user: "Should I create safety checkpoint (git push) first?"
- Do NOT proceed without user confirmation

**Integration with Existing Tools**:
- Feng Shui automated fixes: Check git state first
- Gu Wu Phase 4 (lifecycle changes): Check git state first
- Any file migration/rename: Check git state first

**User Constraint**: User prefers batch commits, but safety > preference for critical ops

---

## üéØ PRIORITY 1: ESSENTIAL WORKFLOWS

### 1. Knowledge Graph First ‚≠ê MANDATORY

**Before ANY work, check what we already know:**

```xml
<use_mcp_tool>
  <server_name>github.com/modelcontextprotocol/servers/tree/main/src/memory</server_name>
  <tool_name>search_nodes</tool_name>
  <arguments>{"query": "[relevant search terms]"}</arguments>
</use_mcp_tool>
```

**Why**: Prevents wasting 90 minutes re-investigating known topics

**At session end**: Store new learnings in knowledge graph with complete WHY context (see section 1.1)

---

### 1.1 Knowledge Graph: Capture the WHY ‚≠ê MANDATORY

**When creating KG entities, ALWAYS include these 8 elements:**

1. **WHAT** - The decision/action taken
2. **WHY** - The reasoning and intent behind it ‚≠ê REQUIRED
3. **PROBLEM** - What issue it solved
4. **ALTERNATIVES** - Options considered and why rejected
5. **USER CONSTRAINTS** - Specific user needs/limitations
6. **VALIDATION** - How we confirmed it worked
7. **WARNINGS** - What not to propose in future
8. **CONTEXT** - User's working style, priorities, environment

**User Philosophy**: 
> "I invest time explaining WHY and intent. Preserve that, not just the output."
> "Avoid repeating mistakes and reinventing wheels."

---

### 1.2 User Preferences & Constraints ‚≠ê TRACK CONTINUOUSLY

**Maintain cumulative understanding of user's working style**

**Current Known Preferences**:
- Prefers batch commits over frequent commits
- Values debugging efficiency highly (daily pain point)
- Deploys frontend + backend together always
- Works on Windows (not Mac/Linux)
- SAP environment context
- Time-sensitive - doesn't want to wait
- Senior-level thinking: practical > textbook
- Patience during learning discussions
- Expects explicit rules (.clinerules) to be followed

---

### 2. Knowledge Vault Documentation ‚≠ê MANDATORY

**ALL documentation ‚Üí `docs/knowledge/` vault**

**‚úÖ ALLOWED**:
- Files in `docs/knowledge/` subdirectories
- Module READMEs: `modules/[name]/README.md`
- Links: `[[Document Name]]` format

**‚ùå FORBIDDEN**:
- .md files in root (except `.clinerules`, `PROJECT_TRACKER.md`, `README.md`)
- .md files in `docs/` outside knowledge vault
- Standalone planning/summary docs

**Workflow**:
1. Search existing docs: `<search_files path="docs/knowledge" regex="..."/>`
2. Create with [[wikilinks]] to 3-5 related docs
3. Update `docs/knowledge/INDEX.md`
4. Commit doc + INDEX together

---

### 3. Check Industry Standards First üí°

**Before proposing solutions, validate against industry best practices**

**Process**:
- Research proven approaches using Perplexity MCP tool for current best practices
- Validate user's intuitive proposals against standards
- Use established patterns (don't reinvent)
- Only create custom when no standard exists

**Using Perplexity for Research**:
```xml
<use_mcp_tool>
  <server_name>Perplexity</server_name>
  <tool_name>search_perplexity</tool_name>
  <arguments>
    {
      "query": "repository pattern vs adapter pattern for database abstraction in Python"
    }
  </arguments>
</use_mcp_tool>
```

**When to Use Perplexity**:
- ‚úÖ Researching design patterns and architecture decisions
- ‚úÖ Validating proposed solutions against industry standards
- ‚úÖ Finding best practices for specific technologies (Python, Flask, SAPUI5)
- ‚úÖ Getting current recommendations (not outdated Stack Overflow posts)

**Benefits**: Save time, proven quality, better collaboration, current best practices

---

### 3.1 Infrastructure-First Principle ‚≠ê CRITICAL

**NEVER build infrastructure without immediately integrating it**

**Rule**: When building core infrastructure (registries, resolvers, interfaces):
1. ‚úÖ Design the architecture
2. ‚úÖ Implement the infrastructure
3. ‚úÖ Write tests for infrastructure
4. ‚úÖ **IMMEDIATELY integrate into application code**
5. ‚úÖ Test the integrated system
6. ‚úÖ Only commit when integration is complete

**Why**: 
- Infrastructure without integration = technical debt
- Quick implementations create fragile, hardwired code
- Spending 2 hours on solid architecture > 30 minutes on quick code
- "Later refactoring" never happens - debt accumulates

---

## üéØ PRIORITY 2: DEVELOPMENT STANDARDS

### 4. API-First Development Order ‚≠ê MANDATORY

**Principle**: Implement and stabilize APIs BEFORE creating UX

**Correct Order**:
1. ‚úÖ Design & implement API (business logic)
2. ‚úÖ Write unit tests (100% coverage - see section 7)
3. ‚úÖ Integrate with API Playground (manual testing)
4. ‚úÖ **VERIFY STABLE** - API working perfectly
5. ‚úÖ THEN design UX on top of stable API

**Requirements for "Stable"**:
- ‚úÖ Zero UI dependencies (works in Python/Node.js)
- ‚úÖ Dependency injection (testable)
- ‚úÖ Unit tests passing (100% coverage via Gu Wu)
- ‚úÖ Manual testing via API Playground
- ‚úÖ Error scenarios handled
- ‚úÖ Performance acceptable

**Why This Order Matters**:
- API bugs discovered during UX work = rework both layers
- Stable API first = UX can focus on experience, not fixing API
- Testing catches issues before UX investment
- API Playground validates before committing to UX design

---

### 5. Feng Shui (È£éÊ∞¥) - Multi-Agent Architecture Intelligence ‚≠ê PHASE 4-17

**Philosophy**: "Wind and water" - maintaining harmonious flow in codebase architecture

**What is Feng Shui?**
- **6 Specialized AI Agents** working in parallel for comprehensive analysis
- Like Gu Wu self-optimizes TESTS, Feng Shui self-corrects CODE
- **ReAct Agent** with Planning: Reason ‚Üí Act ‚Üí Observe ‚Üí Reflect
- **Multi-Agent Orchestration**: Parallel execution with conflict detection (6x faster)
- **Pre-commit Hook**: Prevents violations from entering repository
- **Quality Gate**: Validates modules before deployment

**Phase 4-17 Complete (v4.1)** ‚≠ê:
1. ‚úÖ **6 Specialized Agents**: Architecture, Security, UX, Performance, FileOrg, Documentation
2. ‚úÖ **Parallel Execution**: Up to 6x speedup via concurrent agent execution
3. ‚úÖ **Conflict Detection**: Identifies when agents have contradictory recommendations
4. ‚úÖ **Synthesized Planning**: Unified action plan from all agent findings
5. ‚úÖ **Health Scoring**: Overall module health score (0-100) across all dimensions
6. ‚úÖ **Infinite Loop Protection**: Safety limits on file scanning (MAX 10K files)

**The 6 Specialized Agents** (NEW in Phase 4-17):

1. **ArchitectAgent**: DI violations, SOLID principles, coupling analysis
2. **SecurityAgent**: Hardcoded secrets, SQL injection, auth issues
3. **UXArchitectAgent**: SAP Fiori compliance, UI/UX patterns
4. **FileOrganizationAgent**: File structure, misplaced files, obsolete detection
5. **PerformanceAgent**: N+1 queries, nested loops, caching opportunities
6. **DocumentationAgent**: README quality, docstrings, comment coverage

**Intelligent Planning** (Phase 4-16):
- ‚úÖ **5 Automatic Dependency Rules**:
  * Interface changes ‚Üí Implementation updates
  * module.json config ‚Üí Blueprint registration
  * Test creation ‚Üí Coverage validation
  * Schema changes ‚Üí Data migration
  * File operations ‚Üí Import updates
- ‚úÖ Topological sorting (respects dependencies)
- ‚úÖ Parallel group identification (maximizes concurrency)
- ‚úÖ Critical path analysis (bottleneck identification)
- ‚úÖ Sequential vs parallel time estimates

**When to Use Feng Shui**:

**Option 1: Multi-Agent Comprehensive Analysis** ‚≠ê RECOMMENDED
```bash
# Run 6 specialized agents in parallel (6x faster)
python -c "from pathlib import Path; from tools.fengshui.react_agent import FengShuiReActAgent; \
agent = FengShuiReActAgent(); \
report = agent.run_with_multiagent_analysis(Path('modules/knowledge_graph'), parallel=True)"

# Multi-agent system provides:
# 1. Comprehensive analysis across 6 dimensions (Architecture/Security/UX/Performance/FileOrg/Docs)
# 2. Parallel execution (6x speedup vs sequential)
# 3. Conflict detection (identifies contradictory recommendations)
# 4. Synthesized action plan (prioritized, unified)
# 5. Overall health score (0-100 across all dimensions)
```

**Option 2: Autonomous ReAct Agent (Batch Fixes)**
```bash
# Run autonomous agent for automatic violation fixes
python -m tools.fengshui.react_agent --target-score 95 --max-iterations 10

# Agent autonomously:
# 1. Analyzes architecture violations
# 2. Creates optimal execution plan with dependency detection
# 3. Executes fixes in parallel (up to 3x faster)
# 4. Learns from outcomes, improves strategy
# 5. Reports detailed metrics
```

**Option 3: Manual Quality Gate (Per-Module)**
```bash
# Validate specific module before deployment
python tools/fengshui/module_quality_gate.py knowledge_graph
# MUST exit 0 (PASSED) before module goes live
```

**Option 4: Pre-Commit Hook (Automatic)**
- Runs automatically on `git commit`
- Prevents violations from entering repository
- Fast execution (< 1s for typical commits)
- Bypass: `git commit --no-verify` (emergency only)

**AI Workflow with Feng Shui**:

**BEFORE (Manual)** - AI had to manually fix every violation:
```
‚ùå AI: Manually check DI violations
‚ùå AI: Manually check module.json
‚ùå AI: Manually check test locations
‚ùå AI: Manually fix each violation
‚ùå Result: 30-60 min manual work, error-prone
```

**AFTER (Autonomous)** - Feng Shui handles it:
```
‚úÖ User: "Improve architecture quality"
‚úÖ AI: Runs Feng Shui ReAct agent
‚úÖ Agent: Autonomously detects ‚Üí plans ‚Üí fixes ‚Üí validates
‚úÖ Result: 5-10 min automated, reliable, learns over time
```

**AI Should Use Feng Shui When**:
1. ‚úÖ User mentions "architecture quality", "Feng Shui score", "DI violations"
2. ‚úÖ Multiple architecture violations detected (>5 issues)
3. ‚úÖ Module quality gate fails
4. ‚úÖ User requests "batch architecture fixes"
5. ‚úÖ Pre-deployment validation needed
6. ‚úÖ User requests "comprehensive analysis" or "multi-agent review" ‚≠ê NEW
7. ‚úÖ Module has security, UX, or performance concerns ‚≠ê NEW
8. ‚úÖ Need conflict detection across different architectural dimensions ‚≠ê NEW

**AI Should NOT Manually Fix When Feng Shui Can**:
- ‚ùå Don't manually fix DI violations - Feng Shui detects + fixes automatically
- ‚ùå Don't manually check module.json - Quality gate validates
- ‚ùå Don't manually move test files - Feng Shui autofix handles it
- ‚úÖ DO: Run Feng Shui agent, let it handle architecture repairs autonomously

**Feng Shui Benefits**:
- **Faster**: Multi-agent parallel execution (6x speedup), automatic detection
- **Smarter**: Learns from history, improves strategies over time
- **Comprehensive**: 6 specialized agents cover Architecture/Security/UX/Performance/FileOrg/Docs
- **Reliable**: Conflict detection, synthesized planning, dependency-aware execution
- **Zero Manual Work**: Autonomous end-to-end repair workflow
- **Safe**: Infinite loop protection (max 10K files), no memory exhaustion

**Reference**: `tools/fengshui/react_agent.py`, `tools/fengshui/agents/orchestrator.py`, [[Feng Shui Phase 4-17 Multi-Agent]], `docs/FENG_SHUI_ROUTINE_REQUIREMENTS.md`

---

### 6. Modular Architecture ‚≠ê PROJECT STANDARD

**ENFORCEMENT**: Use Feng Shui quality gate (see section 5)

**Complete Module Structure** (ALL components REQUIRED):
```
modules/[module-name]/
‚îú‚îÄ‚îÄ module.json          # MUST include backend.blueprint if backend/ exists
‚îú‚îÄ‚îÄ backend/             # Python services
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py      # MUST export blueprint
‚îÇ   ‚îú‚îÄ‚îÄ api.py           # MUST define Blueprint()
‚îÇ   ‚îî‚îÄ‚îÄ service.py
‚îú‚îÄ‚îÄ tests/               # Module tests (see section 7)
‚îú‚îÄ‚îÄ docs/                # Module docs (optional)
‚îî‚îÄ‚îÄ README.md            # Module documentation
```

**Validation**:
```bash
# Option 1: Feng Shui agent (autonomous, batch mode)
python -m tools.fengshui.react_agent --modules knowledge_graph

# Option 2: Manual quality gate (per-module)
python tools/fengshui/module_quality_gate.py [module_name]
```

**Validation Rules** (Enforced by Feng Shui):
- ‚úÖ Module Quality Gate MUST pass (exit 0)
- ‚úÖ No DI violations (.connection, .service, .db_path access)
- ‚úÖ No direct module imports (loose coupling)
- ‚úÖ Blueprint self-registers via module.json
- ‚úÖ Uses interfaces from core.interfaces

**Let Feng Shui handle validation** - it's faster and more reliable than manual checks

---

### 7. Gu Wu (È°æÊ≠¶) Testing Framework ‚≠ê MANDATORY - FULLY AUTONOMOUS

**Philosophy**: "Attending to martial affairs with discipline and continuous improvement"

**What is Gu Wu?**
- Self-healing, self-optimizing testing framework
- Like Feng Shui auto-corrects CODE, Gu Wu auto-optimizes TESTS
- Continuously learns from test execution patterns
- Automatically prioritizes, detects flaky tests, suggests improvements
- **Agentic workflow with autonomous test orchestration**

**Phase 7 Complete (Intelligence Layer)** ‚≠ê:
1. ‚úÖ **Test Pyramid**: 70% unit / 20% integration / 10% E2E (enforced automatically)
2. ‚úÖ **100% API Coverage**: All business logic tested
3. ‚úÖ **Self-Learning**: Metrics tracked in SQLite, insights generated
4. ‚úÖ **Auto-Prioritization**: Likely-to-fail tests run first
5. ‚úÖ **Flaky Detection**: Transition-based algorithm (score 0.0-1.0)
6. ‚úÖ **Performance Tracking**: Slow tests flagged (>5s threshold)
7. ‚úÖ **Gap Detection**: Automatically finds untested code
8. ‚úÖ **Meta-Learning**: Learns from execution history
9. ‚úÖ **Intelligence Hub**: 3 engines (Recommendations, Dashboard, Predictive Analytics)

**Test Structure** (REQUIRED for all features):
```
tests/
‚îú‚îÄ‚îÄ unit/                       # 70% of tests (fast, isolated)
‚îÇ   ‚îú‚îÄ‚îÄ modules/[module]/      # Per-module unit tests
‚îÇ   ‚îî‚îÄ‚îÄ core/                  # Core infrastructure tests
‚îú‚îÄ‚îÄ integration/                # 20% of tests (module interactions)
‚îú‚îÄ‚îÄ e2e/                        # 10% of tests (critical workflows)
‚îú‚îÄ‚îÄ guwu/                       # Self-optimization engine
‚îÇ   ‚îú‚îÄ‚îÄ intelligence/          # Phase 7 intelligence engines
‚îÇ   ‚îî‚îÄ‚îÄ agent/                 # Autonomous orchestrator
‚îî‚îÄ‚îÄ conftest.py                 # Shared fixtures & Gu Wu hooks
```

**Writing Tests** (MANDATORY for ALL new code):

```python
import pytest

@pytest.mark.unit
@pytest.mark.fast
def test_feature_succeeds():
    """Test description (AAA pattern)"""
    # ARRANGE
    input_data = {...}
    
    # ACT
    result = my_function(input_data)
    
    # ASSERT
    assert result == expected
```

**Running Tests**:
```bash
# Run all tests with Gu Wu (auto-optimized)
pytest

# Intelligence Hub (recommended - unified interface)
python -m tools.guwu.intelligence.intelligence_hub  # Comprehensive report (all engines)

# Individual engines (for deep analysis)
python -m tools.guwu.intelligence.recommendations   # 8 types of actionable insights
python -m tools.guwu.intelligence.dashboard         # Visual health metrics + trends
python -m tools.guwu.intelligence.predictive        # ML failure forecasting + pre-flight
```

**AI Workflow with Gu Wu Intelligence** ‚≠ê:

**BEFORE (Manual)** - AI had to manually analyze test issues:
```
‚ùå AI: Manually review test failures
‚ùå AI: Guess why tests are flaky
‚ùå AI: No insight into test health trends
‚ùå Result: Reactive, slow debugging
```

**AFTER (Autonomous)** - Gu Wu Intelligence guides AI:
```
‚úÖ AI: Runs Intelligence Hub for comprehensive report
‚úÖ Gu Wu: Provides 8 types of recommendations (prioritized)
‚úÖ Gu Wu: Shows health dashboard with trends
‚úÖ Gu Wu: Predicts likely failures (ML-powered)
‚úÖ Result: Proactive, intelligent test optimization
```

**AI Should Use Gu Wu Intelligence When**:
1. ‚úÖ Test failures occur - check Intelligence Hub for root cause analysis
2. ‚úÖ Before committing code - run pre-flight check to predict failures
3. ‚úÖ Test suite becomes slow - check Dashboard for performance trends
4. ‚úÖ Flaky tests detected - check Recommendations for prioritized fixes
5. ‚úÖ Coverage gaps found - check Recommendations for untested code
6. ‚úÖ User mentions "test health", "flaky tests", "slow tests"
7. ‚úÖ Starting work session - check Dashboard for test suite health

**AI Should NOT Ignore Gu Wu Insights**:
- ‚ùå Don't manually debug flaky tests - Gu Wu identifies patterns automatically
- ‚ùå Don't guess at test priorities - Gu Wu prioritizes by impact
- ‚ùå Don't skip pre-flight checks - Gu Wu predicts failures before they happen
- ‚úÖ DO: Use Intelligence Hub proactively, not reactively

**AI Enforcement** (MANDATORY):
Before using `attempt_completion`, AI must ask in `<thinking>`:
1. ‚ùì Have I written unit tests for all new code?
2. ‚ùì Have I written integration tests for workflows?
3. ‚ùì Have I run tests to verify they pass?
4. ‚ùì Are tests committed with the code?
5. ‚ùì Should I check Gu Wu Intelligence Hub for test health insights?

**If ANY answer is NO**: Write the missing tests FIRST.

**Proactive Intelligence Usage** (RECOMMENDED):
- Run Intelligence Hub at session start to understand test health baseline
- Run pre-flight check before committing to catch predicted failures
- Check recommendations after test failures for root cause guidance
- Review dashboard trends weekly to prevent test debt accumulation

**Reference**: `tests/README.md`, [[Gu Wu Testing Framework]], [[Gu Wu Phase 7 Intelligence]]

---

### 7.1 Test Verification Protocol ‚ö†Ô∏è CRITICAL - NO EXCEPTIONS

**MANDATORY WORKFLOW** (AI MUST follow this sequence):

1. **Write Tests**: Create test file with Gu Wu standards (AAA pattern, pytest marks)
2. **Run Tests**: Execute `pytest [test_file] -v`
3. **Wait for Result**: Do NOT proceed until pytest completes execution
4. **Verify Success**: Check output shows "X passed in Y seconds" (all green)
5. **Fix Failures**: If any tests fail, fix them immediately and re-run
6. **Only Then**: Use `attempt_completion`

**FORBIDDEN SEQUENCES** (AI must NEVER do this):
```
‚ùå Write tests ‚Üí attempt_completion (WITHOUT running pytest)
‚ùå Run pytest ‚Üí attempt_completion (WHILE pytest still collecting)
‚ùå pytest crashes ‚Üí attempt_completion anyway
‚ùå pytest shows failures ‚Üí ignore and use attempt_completion
```

**CORRECT SEQUENCE** (MANDATORY):
```
‚úÖ Write tests (test_*.py)
‚úÖ Run pytest (pytest test_*.py -v)
‚úÖ WAIT for "X passed in Y seconds"
‚úÖ Verify ALL tests green (no failures)
‚úÖ User confirms successful execution
‚úÖ THEN attempt_completion
```

**Why This Matters**:
- Tests that don't run = zero value
- Tests that fail = broken code shipped
- Verification = quality assurance
- User expects: "Tests written AND verified"

---

### 7.2 Browser Testing: LAST RESORT ONLY ‚ö†Ô∏è CRITICAL

**RULE**: Use `browser_action` tool ONLY as absolute last resort

**WHY Browser Testing Should Be Avoided**:
- ‚ùå Inefficient: 60-300 seconds vs 1-5 seconds for pytest
- ‚ùå Unreliable: Can crash system, resource-intensive
- ‚ùå Not automatable: Cannot run in CI/CD pipelines
- ‚ùå Wastes time: Manual clicking vs automated validation
- ‚ùå System stability risk: User's primary concern

**CORRECT Testing Methods (Try These FIRST)**:
1. **Gu Wu/pytest**: Use `pytest tests/` (1-5 seconds) ‚≠ê PREFERRED
2. **Direct API Testing**: Use `test_api_endpoints.py` (5 seconds) ‚≠ê PREFERRED
3. **Unit Tests**: Automated, fast, reliable ‚≠ê PREFERRED
4. **Integration Tests**: Module interaction tests ‚≠ê PREFERRED

**When Browser Testing May Be Considered**:
- ‚úÖ ONLY after ALL other testing methods exhausted
- ‚úÖ ONLY for final UX validation AFTER API is stable

---

### 8. Shi Fu (Â∏àÂÇÖ) - Quality Ecosystem Orchestrator ‚≠ê PHASE 1 COMPLETE (v4.2)

**Philosophy**: "The Master Teacher" - Code and Tests are Yin and Yang

**What is Shi Fu?**
- **Meta-Agent** that observes both Feng Shui (code quality) and Gu Wu (test quality)
- **Cross-Domain Intelligence**: Detects patterns invisible to individual tools
- **Holistic Insights**: Finds root causes spanning both code and tests
- **Non-Invasive**: Read-only observation, guides rather than commands

**Phase 1 Complete (v4.2)** ‚úÖ:
1. ‚úÖ **4 Core Components**: FengShuiInterface, GuWuInterface, EcosystemAnalyzer, CorrelationEngine
2. ‚úÖ **5 Pattern Detectors**: DI‚ÜíFlaky, Complexity‚ÜíCoverage, Security‚ÜíGaps, Performance‚ÜíSlow, Module‚ÜíTest
3. ‚úÖ **21 Comprehensive Tests**: 100% passing (0.92s execution)
4. ‚úÖ **Holistic Health Scoring**: Ecosystem score with correlation penalties
5. ‚úÖ **Teaching Generation**: Actionable recommendations with root cause analysis

**The 5 Correlation Patterns** (Detects Root Causes):

1. **DI Violations ‚Üí Flaky Tests** (URGENT/HIGH):
   - Evidence: Hardwired dependencies cause non-deterministic test behavior
   - Wisdom: "Fix DI violations, flaky tests heal automatically"

2. **High Complexity ‚Üí Low Coverage** (HIGH):
   - Evidence: Complex code has many paths, hard to test thoroughly
   - Wisdom: "Simplify code, testing becomes easier, coverage rises"

3. **Security Issues ‚Üí Test Gaps** (URGENT):
   - Evidence: Vulnerabilities lack regression prevention
   - Wisdom: "Add security tests, vulnerabilities stay fixed"

4. **Performance Issues ‚Üí Slow Tests** (MEDIUM):
   - Evidence: Inefficient code (N+1 queries) slows test suite
   - Wisdom: "Optimize code, tests run faster, better DX"

5. **Module Health ‚Üí Test Health** (HIGH):
   - Evidence: Modules with high violation count have failing tests
   - Wisdom: "Technical debt affects both code and tests together"

**When to Use Shi Fu**:

```bash
# Weekly comprehensive analysis (recommended)
python -m tools.shifu.shifu --weekly-analysis

# Quick ecosystem health check
python -m tools.shifu.shifu --health-check
```

**AI Workflow with Shi Fu** ‚≠ê:

**BEFORE (Isolated)** - Each tool worked alone:
```
‚ùå Feng Shui: "10 DI violations found"
‚ùå Gu Wu: "5 flaky tests detected"
‚ùå User: "Are these related?" (manual correlation)
‚ùå Result: Fix both separately (double work)
```

**AFTER (Holistic)** - Shi Fu connects the dots:
```
‚úÖ Shi Fu: "DI violations CAUSE flaky tests (confidence: 0.9)"
‚úÖ Teaching: "Fix 10 DI violations ‚Üí 5 flaky tests heal automatically"
‚úÖ User: Fixes ONE root cause, both problems solved
‚úÖ Result: Half the work, deeper understanding
```

**AI Should Use Shi Fu When**:
1. ‚úÖ User mentions "quality ecosystem", "holistic view", "why are tests flaky?"
2. ‚úÖ Both Feng Shui and Gu Wu show issues (potential correlation)
3. ‚úÖ User wants to understand relationships between code and test quality
4. ‚úÖ Weekly quality review sessions
5. ‚úÖ Before major refactoring (assess current state)
6. ‚úÖ After implementing features (validate no quality degradation)

**AI Should NOT**:
- ‚ùå Use Shi Fu for isolated code or test issues (use Feng Shui/Gu Wu directly)
- ‚ùå Run Shi Fu on every commit (it's a meta-analysis tool, not per-commit)
- ‚úÖ DO: Use Shi Fu for strategic quality insights, not tactical fixes

**Shi Fu Benefits**:
- **Holistic**: Sees patterns across code + tests (meta-level intelligence)
- **Root Cause**: Identifies underlying issues, not just symptoms
- **Efficient**: Fix one root cause, multiple problems resolve
- **Non-Invasive**: Guides with wisdom, doesn't mandate changes
- **Learning**: Each insight includes WHY + HOW + VALUE

**Integration with Other Tools**:
```
Feng Shui ‚Üí Analyzes code architecture
    ‚Üì
Gu Wu ‚Üí Analyzes test quality
    ‚Üì
Shi Fu ‚Üí Finds correlations, provides holistic wisdom
    ‚Üì
User ‚Üí Makes informed decisions with complete picture
```

**Reference**: `tools/shifu/shifu.py`, [[Shi Fu Phase 1 Implementation]], `tests/unit/tools/test_shifu.py`

---

### 9. SAP Fiori Compliance ‚≠ê MANDATORY

**Critical Rules**:
1. ‚úÖ **Use standard controls first** (InputListItem, not CustomListItem)
2. ‚úÖ **Use built-in properties for sizing** (width="100%", contentWidth="600px")
3. ‚úÖ **No CSS for layout/sizing** - CSS only for colors, animations, branding
4. ‚úÖ **No CSS hacks** (no !important for layout, no custom padding overrides)
5. ‚úÖ **Standard layout containers** (sap.m.Bar, VBox, HBox with standard spacing)
6. ‚úÖ **Pure JavaScript preferred** (easier debugging than XML)
7. ‚úÖ **Start simple** (title + one control ‚Üí incremental complexity)

**AI Enforcement** (MANDATORY):
When implementing UI components, AI must ask in `<thinking>`:
1. ‚ùì Am I using standard SAPUI5 controls (sap.m.*)?
2. ‚ùì Am I using built-in properties for sizing (width, height, contentWidth)?
3. ‚ùì Is my CSS ONLY for colors/animations, NOT layout/sizing?
4. ‚ùì Am I using standard layout containers (Bar, VBox, HBox)?

**If ANY answer is NO**: Refactor to use standard controls and properties

**Reference**: [[SAP Fiori Design Standards]], [[InputListItem Control Decision]]

---

## üéØ PRIORITY 3: OPERATIONAL PRACTICES

### 10. Test Server Cleanup ‚ö†Ô∏è MANDATORY

**CRITICAL RULE**: Always kill test servers after debugging/testing sessions

**MANDATORY WORKFLOW**:

**After testing/debugging complete, BEFORE asking user to verify**:
```bash
# Kill all Python processes (test servers)
taskkill /F /IM python.exe  # Windows
pkill python                # Linux/Mac
```

**When to do this** (ALL these scenarios):
- ‚úÖ After extensive debugging sessions
- ‚úÖ After multiple test server restarts
- ‚úÖ Before asking user to verify results

**Never Skip This**: Server cleanup is NOT optional, it's MANDATORY

---

### 11. Application Logging

**Purpose**: Primarily for AI assistant troubleshooting

**Log**:
- ‚úÖ API calls, database queries, errors
- ‚úÖ Performance metrics, business decisions
- ‚ùå Sensitive data, debug statements

**When issues occur**: Check logs FIRST via UI "Logs" button

---

### 12. Git Workflow & Auto-Archive ‚≠ê AUTOMATED

**AI Can Execute Git Commands ONLY When User Explicitly Requests**:
- ‚úÖ When user says "git push with tag vX.X" ‚Üí AI executes full workflow
- ‚úÖ When user says "commit this" or "push this" ‚Üí AI executes
- ‚úÖ Otherwise: AI stages files (`git add`), user handles commit/push

**Default Behavior** (without explicit request):
- ‚úÖ AI stages: `git add <files>`
- ‚úÖ User commits (prefers batch commits)
- ‚úÖ User tags and pushes

**Exception**: User's explicit request overrides default (e.g., "git push with tag v4.7")

**Git Tag Handling** ‚ö†Ô∏è CRITICAL:
- ‚úÖ If tag already exists (e.g., v4.3), increment to next version (v4.4, v4.5, etc.)
- ‚ùå NEVER delete existing tags
- ‚ùå NEVER force-push tags (`--force` forbidden)
- ‚úÖ Always use incremental versioning
- ‚úÖ Rule: Existing tags are permanent history markers

**Pre-Commit Hook (Feng Shui)** - ACTIVE ‚úÖ:
- ‚úÖ Validates staged files before commit (< 1s)
- ‚úÖ Prevents Feng Shui violations from entering repository
- ‚úÖ Bypass: `git commit --no-verify` (emergency only)

**Tag-Triggered Auto-Archive**:
When user says "git push with tag vX.X", AI AUTOMATICALLY archives PROJECT_TRACKER.md

---

### 13. Proposal Accountability ‚≠ê MANDATORY (NEW - v4.31)

**CRITICAL RULE**: Every proposal, architectural decision, or "TODO" MUST be tracked in PROJECT_TRACKER.md

**The Problem We're Preventing**:
- ‚ùå Proposals written, never implemented (lost in docs)
- ‚ùå "We should do X someday" ‚Üí never tracked ‚Üí forgotten
- ‚ùå TODOs in commit messages ‚Üí no accountability
- ‚ùå Architectural decisions ‚Üí no follow-up

**ENFORCEMENT** (AI MUST follow):

**When user discusses enhancement/fix/proposal** (ANY scope):
1. ‚úÖ Create proposal doc in `docs/knowledge/` (technical details)
2. ‚úÖ **IMMEDIATELY add to PROJECT_TRACKER.md** (accountability)
3. ‚úÖ Assign priority (P0/P1/P2/P3)
4. ‚úÖ Estimate effort (even if rough: "2-3 hours", "1-2 weeks")
5. ‚úÖ Link proposal doc from tracker entry

**What Counts as "Proposal"** (MUST be tracked):
- ‚úÖ Architecture changes ("Let's refactor X")
- ‚úÖ New features ("We should add Y")
- ‚úÖ Quality improvements ("Z could be better")
- ‚úÖ Performance optimizations ("This is slow")
- ‚úÖ Bug investigations ("I found issue W")
- ‚úÖ Technical debt ("We need to clean up V")
- ‚úÖ Process improvements ("Testing could be automated")

**What Doesn't Need Tracking** (immediate execution):
- ‚ùå Bug fixes (fixed immediately, committed same day)
- ‚ùå Trivial changes (typo fixes, formatting)
- ‚ùå Clarifications (answering user questions)

**AI Enforcement Checklist**:

After ANY substantial discussion (>15 min), AI must ask in `<thinking>`:
1. ‚ùì Did we discuss a change/enhancement/proposal?
2. ‚ùì Does it require >1 hour effort?
3. ‚ùì Is it NOT being implemented immediately?
4. ‚ùì Is there a proposal doc created?

**If ALL answers are YES**: Add to PROJECT_TRACKER.md BEFORE moving on

**Exception**: User says "just document it, don't track" ‚Üí Respect user override

**Benefits**:
- ‚úÖ Nothing gets lost
- ‚úÖ Priorities visible (compete for attention)
- ‚úÖ Effort estimates guide planning
- ‚úÖ User can review/reprioritize
- ‚úÖ Future AI sessions see planned work

**Example Workflow** (Correct):
```
User: "Frontend testing should be enforced"
AI: [Creates proposal doc] ‚úÖ
AI: [Adds to PROJECT_TRACKER.md as P2 task] ‚úÖ
AI: "Added WP-UX to tracker (11-15h, P2 priority)"
```

**Example Workflow** (Wrong - OLD behavior):
```
User: "Frontend testing should be enforced"
AI: [Creates proposal doc] ‚úÖ
AI: "I've documented the proposal"
[Never tracked ‚Üí forgotten] ‚ùå
```

**Integration with Existing Tools**:
- Shi Fu weekly analysis ‚Üí Auto-adds HIGH priority items to tracker (Phase 4)
- Feng Shui findings ‚Üí Manual review, add if multi-hour fix needed
- User discussions ‚Üí AI adds immediately

**Reference**: This rule created after identifying WP-UX tracking gap (Feb 12, 2026)

---

### 14. Project Tracker ‚≠ê STREAMLINED FORMAT (v4.1)

**Philosophy**: "Git tags ARE the archive. PROJECT_TRACKER.md is the quick reference."

**New Design** (Feb 6, 2026):
- **Length**: ~150 lines (was 500+ lines) - 70% reduction
- **Content**: Current state, active tasks, quick commands only
- **Archive**: Detailed work in git tag messages (not tracker file)
- **Benefits**: Fast AI resume (2 min vs 10 min), complete history preserved

**What Goes Where**:

**PROJECT_TRACKER.md** (Quick Reference - ~150 lines):
- ‚úÖ QUICK START: Current state (what's working now)
- ‚úÖ ACTIVE TASKS: What to work on next
- ‚úÖ VERSION HISTORY: Recent tags with brief summaries
- ‚úÖ ROADMAP: High-level phases
- ‚úÖ DEVELOPMENT STANDARDS: Quick commands
- ‚úÖ KNOWN ISSUES: Current blockers
- ‚úÖ CRITICAL LESSONS: Avoid repeating mistakes

**Git Tag Messages** (Detailed Archive):
- ‚úÖ Complete implementation details
- ‚úÖ All files changed with purpose
- ‚úÖ Performance metrics and validation
- ‚úÖ Design decisions and WHY
- ‚úÖ Work package completion notes
- ‚úÖ Learnings and insights

**AI Workflow**:

**During Work**:
1. Update ACTIVE TASKS in PROJECT_TRACKER.md (mark complete: [ ] ‚Üí [x])
2. Update QUICK START if major state change
3. Update KNOWN ISSUES if new issues discovered

**After Completion**:
1. User commits with detailed message explaining WHAT was done
2. User creates annotated tag with comprehensive notes:
   ```bash
   git tag -a v4.2 -m "Feature X Implementation
   
   WHAT: Implemented feature X with Y capability
   
   WHY: Solves problem Z for users
   
   IMPLEMENTATION:
   - Added file A for purpose B
   - Modified file C to integrate D
   - Created tests E covering scenarios F
   
   PERFORMANCE:
   - Before: X seconds
   - After: Y seconds
   - Improvement: Z%
   
   VALIDATION:
   - All tests passing (X/X)
   - Module quality gate passed
   - API tested via Playground
   
   LEARNINGS:
   - Insight 1
   - Insight 2"
   ```
3. User pushes: `git push origin main --tags`

**Accessing Details**:
```bash
# View specific version
git show v3.34

# List recent tags
git tag -l --sort=-creatordate | head -10

# Search across tags
git tag -l | xargs -I {} git show {} | grep "search_term"
```

**Benefits**:
- **70% Faster Resume**: AI reads 150 lines vs 500+ lines
- **Complete History**: Nothing lost (in git tags)
- **Easy Search**: `git show [tag]` retrieves full context
- **No Memento Effect**: Tag messages preserve WHY and context
- **Clean Separation**: Overview (tracker) vs Details (tags)

**AI Enforcement** (MANDATORY):

When user says "commit and tag", AI must:
1. ‚úÖ Update PROJECT_TRACKER.md (ACTIVE TASKS, current state if needed)
2. ‚úÖ Stage files: `git add PROJECT_TRACKER.md [other_files]`
3. ‚úÖ Tell user to create detailed annotated tag with full implementation notes

**Update Frequency**:
- ACTIVE TASKS: After each feature/bug completion
- QUICK START: When major state changes (new modules, architecture shifts)
- VERSION HISTORY: When user creates git tag (AI adds reference)
- KNOWN ISSUES: When issues discovered or resolved

---

## üìã AI ASSISTANT CHECKLIST

**Before implementing features**:

1. ‚úÖ Check knowledge graph for existing context
2. ‚úÖ Check knowledge vault docs for guidelines
3. ‚úÖ **ASK: Should I implement discussed architecture first?**
4. ‚úÖ Create compliance checklist (all requirements)
5. ‚úÖ Estimate FULL time (implementation + tests + docs)
6. ‚úÖ Get user approval for approach
7. ‚úÖ Implement with Gu Wu tests (section 7)
8. ‚úÖ **Consider: Should Feng Shui handle architecture fixes?** (section 5)
9. ‚úÖ Update PROJECT_TRACKER.md (ACTIVE TASKS)
10. ‚úÖ Self-audit before staging files

**NO SHORTCUTS**: Implement fewer features correctly, not more features poorly

---

## üéì Quick Reference

### Feng Shui (v4.1) - Multi-Agent Architecture Intelligence ‚≠ê
**Natural Language CLI**: `python -m tools.fengshui [command]` ‚≠ê NEW

**Quick Commands**:
- `python -m tools.fengshui analyze` - Multi-agent analysis (6 agents, parallel)
- `python -m tools.fengshui analyze --module knowledge_graph_v2` - Single module
- `python -m tools.fengshui fix` - Autonomous ReAct agent batch fixes
- `python -m tools.fengshui gate --module data_products_v2` - Quality gate
- `python -m tools.fengshui critical` - Security check only
- `python -m tools.fengshui pre-commit` - Pre-commit validation (< 2s)
- `python -m tools.fengshui pre-push` - Pre-push validation (35-80s)

**Legacy Commands** (still supported):
- `python -m tools.fengshui.react_agent --target-score 95` - Direct ReAct agent
- `python tools/fengshui/module_quality_gate.py [module]` - Direct quality gate

**Capabilities**: 6 Agents + ReAct + Reflection + Planning + Conflict Detection
**Speed**: Up to 6x faster with multi-agent parallel execution
**Safety**: Infinite loop protection (max 10K file scan limit)
Check: [[Feng Shui Phase 4-17]], `tools/fengshui/__main__.py`, `docs/FENG_SHUI_ROUTINE_REQUIREMENTS.md`

### Gu Wu Testing (ACTIVE - Phase 7 Intelligence) ‚≠ê
**Natural Language CLI**: `python -m tools.guwu [command]` ‚≠ê NEW

**Quick Commands**:
- `python -m tools.guwu run` - Run all tests with optimization
- `python -m tools.guwu run tests/unit/ -v` - Run specific tests
- `python -m tools.guwu intelligence` - Intelligence Hub (all 3 engines)
- `python -m tools.guwu dashboard` - Visual health metrics + trends
- `python -m tools.guwu recommend` - Actionable recommendations (8 types)
- `python -m tools.guwu predict` - ML failure forecasting
- `python -m tools.guwu predict --pre-flight` - Pre-commit failure check
- `python -m tools.guwu gaps` - Test coverage gaps

**Legacy Commands** (still supported):
- `pytest` - Direct pytest execution (auto-optimized via hooks)
- `python -m tests.guwu.intelligence.intelligence_hub` - Direct Intelligence Hub

**Structure**: 70% unit / 20% integration / 10% E2E
**Coverage**: 70% minimum (enforced)
Check: `tests/README.md`, `tools/guwu/__main__.py`, [[Gu Wu Phase 7 Intelligence]]

### Shi Fu (v4.8) - Quality Ecosystem Orchestrator ‚≠ê PHASE 8 COMPLETE
**Natural Language CLI**: `python -m tools.shifu [command]` (already implemented)

**Quick Commands**:
- `python -m tools.shifu --session-start` ‚≠ê AUTOMATIC (runs at session start)
- `python -m tools.shifu --weekly-analysis` - Manual weekly analysis
- `python -m tools.shifu --health-check` - Quick health check

**Philosophy**: "Code and Tests are Yin and Yang"
**Patterns**: 5 cross-domain detectors with priority scoring
**Auto-Integration**: Weekly checks + tracker updates (Phase 4)

**üîî AI ROUTINE** (AUTOMATIC - runs at session start):

The Cline Integration (Phase 4) **automatically**:
1. ‚úÖ Checks if 7 days passed since last analysis
2. ‚úÖ Runs weekly analysis if needed
3. ‚úÖ Generates prioritized teachings
4. ‚úÖ Notifies if URGENT/HIGH patterns detected
5. ‚úÖ Updates PROJECT_TRACKER.md with high-priority items
6. ‚úÖ Presents top recommendations to user

**AI Action**: Simply run `--session-start` at beginning of session:
```bash
python -m tools.shifu.shifu --session-start
```

**AI Response Handling**:
- If **healthy**: "‚úÖ Ecosystem health looks good! Continue current work."
- If **issues found**: Present recommendations, ask: "Should I work on [pattern]?"
- If **URGENT**: Recommend addressing immediately before other work
- **Auto-updated tracker**: High-priority items already added to PROJECT_TRACKER.md

**Manual Override** (only if needed):
```bash
# Force full analysis (bypass 7-day check)
python -m tools.shifu.shifu --weekly-analysis

# Quick health check (no full analysis)
python -m tools.shifu.shifu --health-check
```

**AI Enforcement**:
- ‚úÖ DO: Run `--session-start` at beginning of each session
- ‚úÖ DO: Present recommendations if patterns detected
- ‚úÖ DO: Propose working on URGENT patterns first
- ‚ùå DON'T: Manually add items to tracker (Phase 4 does this)
- ‚ùå DON'T: Run weekly analysis manually (use --session-start)

### Architecture
- **Validation**: Feng Shui agent (autonomous) or quality gate (manual)
- **Structure**: `modules/[name]/backend/`
- **Feature toggleable** via module.json
- Check: [[Modular Architecture]]

### Documentation
- **Knowledge vault**: `docs/knowledge/`
- **Use [[wikilinks]]** for cross-references
- **Update INDEX.md** with every new doc

### Git
- **AI stages**, user commits/pushes (batch preference)
- **Detailed notes in git tags** (not PROJECT_TRACKER.md)
- **Pre-commit hook** prevents Feng Shui violations

---

## üìö Key References

**Knowledge Vault** (‚≠ê Start here):
- `docs/knowledge/INDEX.md` - All documentation
- [[Feng Shui Phase 4-16]] - Autonomous architecture improvement
- [[Gu Wu Testing Framework]] - Testing standards
- [[SAP Fiori Design Standards]] - UI guidelines
- [[Modular Architecture]] - Module structure

**Project Files**:
- Feng Shui: `tools/fengshui/react_agent.py`, `tools/fengshui/module_quality_gate.py`
- Testing: `tests/README.md` - Gu Wu guide
- Configuration: `pytest.ini` - Test config

---

**Summary**: 
1. **Priority 0** (Safety): Git checkpoint before critical changes
2. **Priority 1** (Essential): Knowledge graph, docs, standards research, **Shi Fu weekly analysis** ‚≠ê
3. **Priority 2** (Development): API-first, Feng Shui architecture, Gu Wu testing, Shi Fu ecosystem
4. **Priority 3** (Operations): Server cleanup, logging, git workflow

**NEW in v4.8**: Shi Fu (Â∏àÂÇÖ) Phase 3 COMPLETE - Wisdom Generator transforms correlation patterns into prioritized, actionable teachings! Run `python -m tools.shifu.shifu --weekly-analysis` regularly to discover quality insights and add high-priority items to PROJECT_TRACKER.md. "The student sees problems. The master sees patterns. The student fixes symptoms. The master heals roots." üßò‚Äç‚ôÇÔ∏è
