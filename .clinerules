# Development Standards & Best Practices

**Version**: 3.0  
**Last Updated**: 2026-02-05 (Gu Wu Testing Framework Integration)
**Purpose**: Core standards for P2P Data Products development

---

## ‚ö†Ô∏è CRITICAL: ARCHITECTURE-FIRST ENFORCEMENT

**MANDATORY RULE**: When user discusses architecture for 90+ minutes, IMPLEMENT ARCHITECTURE FIRST!

### The Problem We're Preventing
- ‚ùå User spends 90 min discussing modular architecture
- ‚ùå AI ignores discussion, implements features with inline/hardwired code
- ‚ùå Result: Double work (implement ‚Üí refactor), fragile code, technical debt
- ‚ùå User frustration: "We could have avoided this!"

### The Correct Approach
- ‚úÖ User discusses architecture for 90+ minutes ‚Üí **THAT IS THE PRIORITY**
- ‚úÖ AI asks: "Should I implement the architecture discussed before adding features?"
- ‚úÖ AI implements: Interfaces ‚Üí Modules ‚Üí Integration ‚Üí THEN features
- ‚úÖ Result: Features built on solid foundation, zero refactoring needed

### Enforcement Checklist (AI MUST DO THIS)
Before implementing ANY feature, AI must ask in `<thinking>`:
1. ‚ùì Has user discussed architecture extensively (60+ minutes)?
2. ‚ùì Are there unimplemented architectural concepts (interfaces, registries, DI)?
3. ‚ùì Am I about to hardwire code that should use the discussed architecture?
4. ‚ùì Will this create technical debt that requires later refactoring?

**If ANY answer is YES**: STOP. Ask user: "Should I implement [architecture concept] first?"

---

## üéØ PRIORITY 0: SAFETY CHECKPOINT ‚≠ê CRITICAL

### 0. Clean Git State Before Critical Changes ‚ö†Ô∏è MANDATORY

**RULE**: Before any risky/critical operation, ensure clean git state with remote backup

**What Qualifies as "Critical Changes"**:
- ‚úÖ Architecture refactoring (moving files, renaming modules)
- ‚úÖ Database schema changes (migrations, table modifications)
- ‚úÖ Core infrastructure changes (DI refactoring, interface changes)
- ‚úÖ Build system changes (package.json, requirements.txt)
- ‚úÖ Configuration changes (.gitignore, pytest.ini, feature_flags.json)
- ‚úÖ Multi-file operations (mass renames, bulk updates)

**Safety Checkpoint Workflow**:

```bash
# 1. Check git status
git status

# 2. If changes exist:
git add .
git commit -m "checkpoint: [brief description of current state]"
git push origin main

# 3. Now safe to proceed with critical operation
```

**Why This Matters**:
- Last night's catastrophe: Feng Shui migration crashed mid-way
- Without clean commit: Would have lost work + had inconsistent state
- With clean commit: Can rollback instantly via `git reset --hard HEAD~1`
- User philosophy: "Always have a safe rollback point"

**AI Enforcement** (MANDATORY):

Before ANY critical operation, AI must ask in `<thinking>`:
1. ‚ùì Is this a critical/risky operation (see list above)?
2. ‚ùì Are there uncommitted changes in git?
3. ‚ùì Do I have a clean rollback point?

**If operation is critical AND there are uncommitted changes**: 
- STOP and ask user: "Should I create safety checkpoint (git push) first?"
- Do NOT proceed without user confirmation

**Examples**:

**‚ùå WRONG** (no safety checkpoint):
```
User: "Refactor DI architecture"
AI: *starts moving files immediately*
AI: *crashes after 50% complete*
Result: Inconsistent state, manual cleanup required
```

**‚úÖ CORRECT** (with safety checkpoint):
```
User: "Refactor DI architecture"
AI: "This is a critical operation. Should I create safety checkpoint first?"
User: "Yes"
AI: *git add . && git commit && git push*
AI: *now proceeds with refactoring*
Result: If crash occurs, user runs: git reset --hard HEAD~1
```

**Integration with Existing Tools**:
- Feng Shui Phase 1 (scripts cleanup): Check git state first
- Gu Wu Phase 4 (lifecycle changes): Check git state first
- Any file migration/rename: Check git state first

**User Constraint**: User prefers batch commits, but safety > preference for critical ops

---

## üéØ PRIORITY 1: ESSENTIAL WORKFLOWS

### 1. Knowledge Graph First ‚≠ê MANDATORY

**Before ANY work, check what we already know:**

```xml
<use_mcp_tool>
  <server_name>github.com/modelcontextprotocol/servers/tree/main/src/memory</server_name>
  <tool_name>search_nodes</tool_name>
  <arguments>{"query": "[relevant search terms]"}</arguments>
</use_mcp_tool>
```

**Why**: Prevents wasting 90 minutes re-investigating known topics

**At session end**: Store new learnings in knowledge graph with complete WHY context (see section 1.1)

---

### 1.1 Knowledge Graph: Capture the WHY ‚≠ê MANDATORY

**When creating KG entities, ALWAYS include these 8 elements:**

1. **WHAT** - The decision/action taken
2. **WHY** - The reasoning and intent behind it ‚≠ê REQUIRED
3. **PROBLEM** - What issue it solved
4. **ALTERNATIVES** - Options considered and why rejected
5. **USER CONSTRAINTS** - Specific user needs/limitations
6. **VALIDATION** - How we confirmed it worked
7. **WARNINGS** - What not to propose in future
8. **CONTEXT** - User's working style, priorities, environment

**User Philosophy**: 
> "I invest time explaining WHY and intent. Preserve that, not just the output."
> "Avoid repeating mistakes and reinventing wheels."

---

### 1.2 User Preferences & Constraints ‚≠ê TRACK CONTINUOUSLY

**Maintain cumulative understanding of user's working style**

**Current Known Preferences**:
- Prefers batch commits over frequent commits
- Values debugging efficiency highly (daily pain point)
- Deploys frontend + backend together always
- Works on Windows (not Mac/Linux)
- SAP environment context
- Time-sensitive - doesn't want to wait
- Senior-level thinking: practical > textbook
- Patience during learning discussions
- Expects explicit rules (.clinerules) to be followed

---

### 2. Knowledge Vault Documentation ‚≠ê MANDATORY

**ALL documentation ‚Üí `docs/knowledge/` vault**

**‚úÖ ALLOWED**:
- Files in `docs/knowledge/` subdirectories
- Module READMEs: `modules/[name]/README.md`
- Links: `[[Document Name]]` format

**‚ùå FORBIDDEN**:
- .md files in root (except `.clinerules`, `PROJECT_TRACKER.md`, `README.md`)
- .md files in `docs/` outside knowledge vault
- Standalone planning/summary docs

**Workflow**:
1. Search existing docs: `<search_files path="docs/knowledge" regex="..."/>`
2. Create with [[wikilinks]] to 3-5 related docs
3. Update `docs/knowledge/INDEX.md`
4. Commit doc + INDEX together

---

### 3. Check Industry Standards First üí°

**Before proposing solutions, validate against industry best practices**

**Process**:
- Research proven approaches (Google/Perplexity for current best practices)
- Validate user's intuitive proposals against standards
- Use established patterns (don't reinvent)
- Only create custom when no standard exists

**Benefits**: Save time, proven quality, better collaboration

---

### 3.1 Infrastructure-First Principle ‚≠ê CRITICAL

**NEVER build infrastructure without immediately integrating it**

**Rule**: When building core infrastructure (registries, resolvers, interfaces):
1. ‚úÖ Design the architecture
2. ‚úÖ Implement the infrastructure
3. ‚úÖ Write tests for infrastructure
4. ‚úÖ **IMMEDIATELY integrate into application code**
5. ‚úÖ Test the integrated system
6. ‚úÖ Only commit when integration is complete

**Why**: 
- Infrastructure without integration = technical debt
- Quick implementations create fragile, hardwired code
- Spending 2 hours on solid architecture > 30 minutes on quick code
- "Later refactoring" never happens - debt accumulates

---

## üéØ PRIORITY 2: DEVELOPMENT STANDARDS

### 4. API-First Development Order ‚≠ê MANDATORY

**Principle**: Implement and stabilize APIs BEFORE creating UX

**Correct Order**:
1. ‚úÖ Design & implement API (business logic)
2. ‚úÖ Write unit tests (100% coverage - see section 6)
3. ‚úÖ Integrate with API Playground (manual testing)
4. ‚úÖ **VERIFY STABLE** - API working perfectly
5. ‚úÖ THEN design UX on top of stable API

**Requirements for "Stable"**:
- ‚úÖ Zero UI dependencies (works in Python/Node.js)
- ‚úÖ Dependency injection (testable)
- ‚úÖ Unit tests passing (100% coverage via Gu Wu)
- ‚úÖ Manual testing via API Playground
- ‚úÖ Error scenarios handled
- ‚úÖ Performance acceptable

**Why This Order Matters**:
- API bugs discovered during UX work = rework both layers
- Stable API first = UX can focus on experience, not fixing API
- Testing catches issues before UX investment
- API Playground validates before committing to UX design

---

### 5. Modular Architecture ‚≠ê PROJECT STANDARD

**CRITICAL ENFORCEMENT**: Run quality gate before ANY module is considered complete:
```bash
python tools/fengshui/module_quality_gate.py [module_name]
# MUST exit 0 (PASSED) before module goes live
```

**Complete Module Structure** (ALL components REQUIRED):
```
modules/[module-name]/
‚îú‚îÄ‚îÄ module.json          # MUST include backend.blueprint if backend/ exists
‚îú‚îÄ‚îÄ backend/             # Python services
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py      # MUST export blueprint
‚îÇ   ‚îú‚îÄ‚îÄ api.py           # MUST define Blueprint()
‚îÇ   ‚îî‚îÄ‚îÄ service.py
‚îú‚îÄ‚îÄ tests/               # Module tests (see section 6)
‚îú‚îÄ‚îÄ docs/                # Module docs (optional)
‚îî‚îÄ‚îÄ README.md            # Module documentation
```

**Validation Rules**:
- ‚úÖ Module Quality Gate MUST pass (exit 0)
- ‚úÖ No DI violations (.connection, .service, .db_path access)
- ‚úÖ No direct module imports (loose coupling)
- ‚úÖ Blueprint self-registers via module.json
- ‚úÖ Uses interfaces from core.interfaces

**NEVER create a module without running the quality gate** - it enforces ALL these principles automatically.

---

### 6. Gu Wu (È°æÊ≠¶) Testing Framework ‚≠ê MANDATORY - FULLY AUTONOMOUS

**Philosophy**: "Attending to martial affairs with discipline and continuous improvement"

**What is Gu Wu?**
- Self-healing, self-optimizing testing framework
- Like Feng Shui auto-corrects CODE, Gu Wu auto-optimizes TESTS
- Continuously learns from test execution patterns
- Automatically prioritizes, detects flaky tests, suggests improvements
- **NOW: Automatically detects missing tests (gap analysis)**

**Phase 1 (Complete ‚úÖ)**:
1. ‚úÖ **Test Pyramid**: 70% unit / 20% integration / 10% E2E (enforced automatically)
2. ‚úÖ **100% API Coverage**: All business logic tested (enforced via pytest)
3. ‚úÖ **Self-Learning**: Metrics tracked in SQLite, insights generated
4. ‚úÖ **Auto-Prioritization**: Likely-to-fail tests run first
5. ‚úÖ **Flaky Detection**: Transition-based algorithm (score 0.0-1.0)
6. ‚úÖ **Performance Tracking**: Slow tests flagged (>5s threshold)

**Phase 2 (Complete ‚úÖ)**:
1. ‚úÖ **Redundancy Detection**: Find duplicate/overlapping tests (`python -m tests.guwu.analyzer redundancy`)
2. ‚úÖ **Smart Test Selection**: Run only tests affected by code changes (`python -m tests.guwu.analyzer smart-select <files>`)

**Phase 3 (Complete ‚úÖ - AUTONOMOUS)**:
1. ‚úÖ **Gap Detection**: Automatically finds untested code after every pytest run
2. ‚úÖ **Zero Configuration**: Integrated into pytest hooks, no manual intervention needed
3. ‚úÖ **CRITICAL Gap Alerts**: Displays top 5 critical gaps automatically
4. ‚úÖ **Silent Operation**: Runs in background, doesn't interrupt workflow

**Test Structure** (REQUIRED for all features):
```
tests/
‚îú‚îÄ‚îÄ unit/                       # 70% of tests (fast, isolated)
‚îÇ   ‚îú‚îÄ‚îÄ modules/[module]/      # Per-module unit tests
‚îÇ   ‚îî‚îÄ‚îÄ core/                  # Core infrastructure tests
‚îú‚îÄ‚îÄ integration/                # 20% of tests (module interactions)
‚îú‚îÄ‚îÄ e2e/                        # 10% of tests (critical workflows)
‚îú‚îÄ‚îÄ guwu/                       # Self-optimization engine
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py             # Metrics collector
‚îÇ   ‚îî‚îÄ‚îÄ [other components]
‚îú‚îÄ‚îÄ conftest.py                 # Shared fixtures & Gu Wu hooks
‚îî‚îÄ‚îÄ README.md                   # Complete testing guide
```

**Test Placement Guidelines** (MANDATORY):
- ‚úÖ **Unit tests**: `tests/unit/modules/[module]/test_*.py` for single-component tests
- ‚úÖ **Integration tests**: `tests/integration/test_*.py` for multi-component interactions
- ‚úÖ **E2E tests**: `tests/e2e/test_*.py` for full user workflows
- ‚ùå **NEVER** place tests in `scripts/python/test_*.py` (wrong location!)
- ‚ùå **NEVER** place tests in `modules/[module]/tests/` (unless module-specific, then ALSO add to main tests/)
- ‚úÖ **Always use pytest format**: `@pytest.mark.unit`, `@pytest.mark.integration`, `@pytest.mark.e2e`
- ‚úÖ **Always use AAA pattern**: Arrange, Act, Assert (commented in test)

**Writing Tests** (MANDATORY for ALL new code):

```python
import pytest

@pytest.mark.unit
@pytest.mark.fast
def test_feature_succeeds():
    """Test description (AAA pattern)"""
    # ARRANGE
    input_data = {...}
    
    # ACT
    result = my_function(input_data)
    
    # ASSERT
    assert result == expected
```

**Running Tests**:
```bash
# Run all tests with Gu Wu
pytest

# Run specific module
pytest tests/unit/modules/knowledge_graph/ -v

# Check coverage
pytest --cov=modules.knowledge_graph

# Run with detailed insights
pytest -v --durations=10
```

**Gu Wu Features (Automatic)**:
- ‚úÖ Metrics collection (every test execution tracked)
- ‚úÖ Flaky test detection (marks intermittent failures)
- ‚úÖ Slow test detection (flags tests >5s)
- ‚úÖ Test prioritization (likely-to-fail run first)
- ‚úÖ Pyramid compliance (validates 70/20/10 distribution)
- ‚úÖ Coverage trending (alerts on drops >5%)
- ‚úÖ Autonomous insights (recommendations at session end)

**AI Enforcement** (MANDATORY):
Before using `attempt_completion`, AI must ask in `<thinking>`:
1. ‚ùì Have I written unit tests for all new code?
2. ‚ùì Have I written integration tests for workflows?
3. ‚ùì Have I run tests to verify they pass?
4. ‚ùì Are tests committed with the code?

**If ANY answer is NO**: Write the missing tests FIRST.

**Gu Wu Runs Automatically**:
- Gap analyzer runs automatically after every `pytest` execution
- Displays CRITICAL gaps that need attention
- Full reports saved to `tests/guwu/gap_analysis_report.txt`
- AI doesn't need to run gap analyzer manually - it's automatic

**User Expectation**: 
> "Don't make me ask for tests. Include them automatically."
> "Tests are part of the deliverable, not an afterthought."
> "Gu Wu should be self-contained and autonomous."

**Reference**: `tests/README.md`, [[Gu Wu Testing Framework]], [[Comprehensive Testing Strategy]]

---

### 6.1 Browser Testing: LAST RESORT ONLY ‚ö†Ô∏è CRITICAL

**RULE**: Use `browser_action` tool ONLY as absolute last resort when all other methods fail

**WHY Browser Testing Should Be Avoided**:
- ‚ùå Inefficient: 60-300 seconds vs 1-5 seconds for pytest
- ‚ùå Unreliable: Can crash system, resource-intensive
- ‚ùå Not automatable: Cannot run in CI/CD pipelines
- ‚ùå Wastes time: Manual clicking vs automated validation
- ‚ùå System stability risk: User's primary concern

**CORRECT Testing Methods (Try These FIRST)**:
1. **Gu Wu/pytest**: Use `pytest tests/` (1-5 seconds) ‚≠ê PREFERRED
2. **Direct API Testing**: Use `test_api_endpoints.py` (5 seconds) ‚≠ê PREFERRED
3. **Unit Tests**: Automated, fast, reliable ‚≠ê PREFERRED
4. **Integration Tests**: Module interaction tests ‚≠ê PREFERRED
5. **CLI Validation**: Use `check_*.py` scripts (instant)

**Time Comparison**:
- ‚úÖ Gu Wu/pytest test: 1-5 seconds, automated, reliable, safe
- ‚ùå Browser test: 60-300 seconds, manual, crash-prone, risky

**When Browser Testing May Be Considered**:
- ‚úÖ ONLY after ALL other testing methods exhausted
- ‚úÖ ONLY when absolutely nothing else works
- ‚úÖ ONLY for final UX validation AFTER API is stable

**User Constraint**: System stability is critical - browser testing causes crashes and wastes time.

---

### 7. SAP Fiori Compliance ‚≠ê MANDATORY

**Critical Rules**:
1. ‚úÖ **Use standard controls first** (InputListItem, not CustomListItem)
2. ‚úÖ **No CSS hacks** (no !important, no custom padding overrides)
3. ‚úÖ **Pure JavaScript preferred** (easier debugging than XML)
4. ‚úÖ **Start simple** (title + one control ‚Üí incremental complexity)

**Quick Check**:
- Which Fiori control for this purpose?
- Standard control available? Use it!
- Need CSS fix? Wrong control chosen!

**Reference**: [[SAP Fiori Design Standards]], [[InputListItem Control Decision]]

---

## üéØ PRIORITY 3: OPERATIONAL PRACTICES

### 8. Test Server Cleanup ‚ö†Ô∏è MANDATORY

**CRITICAL RULE**: Always kill test servers after debugging/testing sessions complete

**The Problem**:
- AI starts multiple parallel test server instances during debugging
- User tests on wrong/stale server, sees old code behavior
- Result: Wasted 30-90 minutes debugging phantom issues

**MANDATORY WORKFLOW**:

**After testing/debugging complete, BEFORE asking user to verify**:
```bash
# Kill all Python processes (test servers)
taskkill /F /IM python.exe  # Windows
pkill python                # Linux/Mac
```

**When to do this** (ALL these scenarios):
- ‚úÖ After extensive debugging sessions
- ‚úÖ After root cause analysis complete
- ‚úÖ After multiple test server restarts
- ‚úÖ Before asking user to verify results
- ‚úÖ When switching from testing to implementation

**Correct Workflow**:
```
Test ‚Üí Debug ‚Üí Fix ‚Üí CLEANUP SERVERS ‚Üê MANDATORY ‚Üí Ask user to verify
```

**Never Skip This**: Server cleanup is NOT optional, it's MANDATORY

---

### 9. Application Logging

**Purpose**: Primarily for AI assistant troubleshooting

**Log**:
- ‚úÖ API calls, database queries, errors
- ‚úÖ Performance metrics, business decisions
- ‚ùå Sensitive data, debug statements

**When issues occur**: Check logs FIRST via UI "Logs" button

---

### 10. Git Workflow & Auto-Archive ‚≠ê AUTOMATED

**AI Does**:
- ‚úÖ Stage: `git add <files>`

**AI Does NOT**:
- ‚ùå Commit (user decides when - prefers batch commits)
- ‚ùå Push to GitHub (user decides when)
- ‚ùå Pull changes
- ‚ùå Remote operations

**User Preference**: Batch commits over frequent commits (see section 1.2)

**Tagging**: User creates tags for major milestones

---

### 10.1 Tag-Triggered Auto-Archive ‚≠ê AUTOMATED WORKFLOW

**RULE**: When user says "git push with tag vX.X", AI AUTOMATICALLY archives PROJECT_TRACKER.md

**Workflow** (AI executes WITHOUT asking):

1. **Detect tag request**: User mentions "tag v3.0" or "git push with tag"
2. **Get last tag**: `git describe --tags --abbrev=0`
3. **Extract milestone work**: All entries since last tag date
4. **Create archive**: `docs/archive/TRACKER-v{version}-{date}.md`
5. **Compress main file**: Keep only Quick Resume (200 lines), remove completed work
6. **Update main file header**: Add archive link
7. **Commit archive changes**: `git add docs/archive/ PROJECT_TRACKER.md`
8. **Then proceed with tag/push**: `git tag -a v{version}` ‚Üí `git push origin main --tags`

**Benefits**: Zero manual work, main file stays lean, complete history preserved

---

### 11. Project Tracker

**Update `PROJECT_TRACKER.md` after**:
- ‚úÖ Feature completion
- ‚úÖ Major refactoring
- ‚úÖ Bug fixes
- ‚úÖ Architecture changes

**Format**: Version entry with objectives, work performed, metrics, status

---

## üìã AI ASSISTANT CHECKLIST

**Before implementing features**:

1. ‚úÖ Check knowledge graph for existing context
2. ‚úÖ Check knowledge vault docs for guidelines
3. ‚úÖ **ASK: Should I implement discussed architecture first?**
4. ‚úÖ Create compliance checklist (all requirements)
5. ‚úÖ Estimate FULL time (implementation + tests + docs)
6. ‚úÖ Get user approval for approach
7. ‚úÖ Implement with Gu Wu tests (section 6)
8. ‚úÖ Run Gu Wu to verify quality
9. ‚úÖ Update PROJECT_TRACKER.md
10. ‚úÖ Self-audit before staging files

**NO SHORTCUTS**: Implement fewer features correctly, not more features poorly

---

## üéì Quick Reference

### Gu Wu Testing (NEW)
- Run: `pytest` (auto-optimized)
- Structure: 70% unit / 20% integration / 10% E2E
- Coverage: 70% minimum (enforced)
- Self-learning: Automatic metrics collection
- Check: `tests/README.md`, [[Gu Wu Testing Framework]]

### Fiori UI Development
- Standard controls first
- No CSS hacks
- Pure JavaScript preferred
- Start simple, build incrementally
- Check: [[SAP Fiori Design Standards]]

### Architecture
- Modular structure: `modules/[name]/backend/`
- Quality gate: `python tools/fengshui/module_quality_gate.py [module]`
- Feature toggleable via module.json
- Self-documented with README.md
- Check: [[Modular Architecture]]

### Documentation
- Knowledge vault: `docs/knowledge/`
- Use [[wikilinks]] for cross-references
- Update INDEX.md with every new doc
- Check: `docs/knowledge/README.md`

### Git
- Clear commit messages
- AI stages, user commits/pushes (batch preference)
- Tags trigger auto-archive of PROJECT_TRACKER.md
- Update PROJECT_TRACKER.md for major changes

---

## üìö Key References

**Knowledge Vault** (‚≠ê Start here):
- `docs/knowledge/INDEX.md` - All documentation
- [[Gu Wu Testing Framework]] - Testing standards
- [[SAP Fiori Design Standards]] - UI guidelines
- [[Modular Architecture]] - Module structure
- [[Comprehensive Testing Strategy]] - Testing guide

**Project Files**:
- Testing: `tests/README.md` - Gu Wu guide
- Configuration: `pytest.ini` - Test config
- Quality: `tools/fengshui/module_quality_gate.py`

**Legacy Docs** (detailed references):
- Fiori: `docs/fiori/SAP_FIORI_DESIGN_GUIDELINES.md`
- HANA: `docs/hana-cloud/` (25+ guides)

---

**Summary**: 
1. **Priority 1** (Essential Workflows): Knowledge graph, docs, standards research
2. **Priority 2** (Development): API-first, modular architecture, Gu Wu testing
3. **Priority 3** (Operations): Server cleanup, logging, git workflow

**NEW in v3.0**: Gu Wu testing framework integration - all tests now self-optimize automatically!