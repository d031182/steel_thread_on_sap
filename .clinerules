# Development Standards & Best Practices

**Version**: 2.2  
**Last Updated**: 2026-01-26 (12:00 AM - WHY Principle & User Preferences Added)
**Purpose**: Core standards for P2P Data Products development

---

## ‚ö†Ô∏è CRITICAL: ARCHITECTURE-FIRST ENFORCEMENT

**MANDATORY RULE**: When user discusses architecture for 90+ minutes, IMPLEMENT ARCHITECTURE FIRST!

### The Problem We're Preventing
- ‚ùå User spends 90 min discussing modular architecture
- ‚ùå AI ignores discussion, implements features with inline/hardwired code
- ‚ùå Result: Double work (implement ‚Üí refactor), fragile code, technical debt
- ‚ùå User frustration: "We could have avoided this!"

### The Correct Approach
- ‚úÖ User discusses architecture for 90+ minutes ‚Üí **THAT IS THE PRIORITY**
- ‚úÖ AI asks: "Should I implement the architecture discussed before adding features?"
- ‚úÖ AI implements: Interfaces ‚Üí Modules ‚Üí Integration ‚Üí THEN features
- ‚úÖ Result: Features built on solid foundation, zero refactoring needed

### Enforcement Checklist (AI MUST DO THIS)
Before implementing ANY feature, AI must ask in `<thinking>`:
1. ‚ùì Has user discussed architecture extensively (60+ minutes)?
2. ‚ùì Are there unimplemented architectural concepts (interfaces, registries, DI)?
3. ‚ùì Am I about to hardwire code that should use the discussed architecture?
4. ‚ùì Will this create technical debt that requires later refactoring?

**If ANY answer is YES**: STOP. Ask user: "Should I implement [architecture concept] first?"

### Examples of Architecture-First Signals
- User discusses "modular architecture" for 90+ minutes ‚Üí Implement modules FIRST
- User discusses "dependency injection" ‚Üí Create interfaces FIRST
- User discusses "plugin system" ‚Üí Create registry FIRST
- User discusses "abstraction layer" ‚Üí Create abstractions FIRST

### Time Math (Why This Matters)
- **Wrong Way**: 90 min discussion + feature implementation + refactoring = **3x effort**
- **Right Way**: 90 min discussion + architecture + features = **1.5x effort**
- **Savings**: 50% less work by doing architecture first!

### Never Say This
- ‚ùå "We can refactor later" (= technical debt)
- ‚ùå "Let's get it working first" (= wrong priority)
- ‚ùå "Architecture is overhead" (= it's INVESTMENT)

### Always Say This
- ‚úÖ "Should I implement the architecture we discussed first?"
- ‚úÖ "Let me create the interfaces before the implementations"
- ‚úÖ "I'll build this on the modular foundation we designed"

---

## üéØ PRIORITY 1: ESSENTIAL WORKFLOWS

### 1. Knowledge Graph First ‚≠ê MANDATORY

**Before ANY work, check what we already know:**

```xml
<use_mcp_tool>
  <server_name>github.com/modelcontextprotocol/servers/tree/main/src/memory</server_name>
  <tool_name>search_nodes</tool_name>
  <arguments>{"query": "[relevant search terms]"}</arguments>
</use_mcp_tool>
```

**Why**: Prevents wasting 90 minutes re-investigating known topics

**At session end**: Store new learnings in knowledge graph

---

### 1.1 Knowledge Graph: Capture the WHY ‚≠ê MANDATORY

**When creating KG entities, ALWAYS include these 8 elements:**

1. **WHAT** - The decision/action taken
2. **WHY** - The reasoning and intent behind it ‚≠ê REQUIRED
3. **PROBLEM** - What issue it solved
4. **ALTERNATIVES** - Options considered and why rejected
5. **USER CONSTRAINTS** - Specific user needs/limitations
6. **VALIDATION** - How we confirmed it worked
7. **WARNINGS** - What not to propose in future
8. **CONTEXT** - User's working style, priorities, environment

**User Philosophy**: 
> "I invest time explaining WHY and intent. Preserve that, not just the output."
> "Avoid repeating mistakes and reinventing wheels."

**Example - WRONG** ‚ùå:
```json
{
  "observation": "Merged frontend into app/static/"
}
```

**Example - RIGHT** ‚úÖ:
```json
{
  "observations": [
    "WHAT: Merged frontend into app/static/",
    "WHY: User deploys together always (constraint)",
    "WHY: Debugging efficiency is daily priority",
    "PROBLEM: Separate folders slow debugging",
    "ALTERNATIVES: Keep separate (rejected - no benefit for user)",
    "USER CONSTRAINT: Single deployment unit",
    "VALIDATION: 2 bugs fixed in 5 min (merged structure helped)",
    "WARNING: Don't suggest separating frontend in future",
    "CONTEXT: User values practical > theoretical architecture"
  ]
}
```

**Benefits**:
- Next AI understands reasoning, not just outputs
- Won't re-propose rejected ideas
- Respects user's time investment in explanations
- Enables true continuous improvement

**Time cost**: +30 seconds per entity  
**Time saved**: Hours of re-explaining in future sessions

---

### 1.2 User Preferences & Constraints ‚≠ê TRACK CONTINUOUSLY

**Maintain cumulative understanding of user's working style**

**When you discover user preferences/constraints, create/update KG entity**:

```json
{
  "name": "User_Working_Preferences",
  "entityType": "user-profile",
  "observations": [
    "Prefers batch commits over frequent commits",
    "Values debugging efficiency highly (daily pain point)",
    "Deploys frontend + backend together always",
    "Works on Windows (not Mac/Linux)",
    "SAP environment context",
    "Time-sensitive - doesn't want to wait",
    "Senior-level thinking: practical > textbook",
    "Patience during learning discussions",
    "Expects explicit rules (.clinerules) to be followed"
  ]
}
```

**Add observations when user**:
- States a preference ("I prefer...")
- Mentions a constraint ("I always...")
- Shows frustration/satisfaction (indicates priority)
- Makes a trade-off decision (reveals values)
- Explains their working environment

**Benefits**:
- Future AI sessions understand user's context
- Propose compatible solutions (no Windows ‚Üí Mac suggestions)
- Respect user's workflow patterns
- Avoid suggesting rejected approaches

**This is separate from technical decisions** - it's about understanding WHO the user is and HOW they work.

---

### 2. Knowledge Vault Documentation ‚≠ê MANDATORY

**ALL documentation ‚Üí `docs/knowledge/` vault**

**‚úÖ ALLOWED**:
- Files in `docs/knowledge/` subdirectories
- Module READMEs: `modules/[name]/README.md`
- Links: `[[Document Name]]` format

**‚ùå FORBIDDEN**:
- .md files in root (except `.clinerules`, `PROJECT_TRACKER.md`, `README.md`)
- .md files in `docs/` outside knowledge vault
- Standalone planning/summary docs

**Workflow**:
1. Search existing docs: `<search_files path="docs/knowledge" regex="..."/>`
2. Create with [[wikilinks]] to 3-5 related docs
3. Update `docs/knowledge/INDEX.md`
4. Commit doc + INDEX together

**Reference**: `docs/knowledge/README.md`

---

### 3. Check Industry Standards First üí°

**Before proposing solutions, validate against industry best practices**

**Process**:
- Research proven approaches
- Validate user's intuitive proposals
- Use established patterns (don't reinvent)
- Only create custom when no standard exists

**Benefits**: Save time, proven quality, better collaboration

### 3.1 Infrastructure-First Principle ‚≠ê CRITICAL

**NEVER build infrastructure without immediately integrating it**

**Rule**: When building core infrastructure (registries, resolvers, interfaces):
1. ‚úÖ Design the architecture
2. ‚úÖ Implement the infrastructure
3. ‚úÖ Write tests for infrastructure
4. ‚úÖ **IMMEDIATELY integrate into application code**
5. ‚úÖ Test the integrated system
6. ‚úÖ Only commit when integration is complete

**Why**: 
- Infrastructure without integration = technical debt
- Quick implementations create fragile, hardwired code
- Spending 2 hours on solid architecture > 30 minutes on quick code
- "Later refactoring" never happens - debt accumulates

**Example**: Building ModuleRegistry without refactoring backend/app.py to use it = WRONG

**Correct**: Build ModuleRegistry AND refactor backend/app.py in SAME session = RIGHT

---

## üéØ PRIORITY 2: DEVELOPMENT STANDARDS

### 4. API-First Development Order ‚≠ê MANDATORY

**Principle**: Implement and stabilize APIs BEFORE creating UX

**Correct Order**:
1. ‚úÖ Design & implement API (business logic)
2. ‚úÖ Write unit tests (100% coverage)
3. ‚úÖ Integrate with API Playground (manual testing)
4. ‚úÖ **VERIFY STABLE** - API working perfectly
5. ‚úÖ THEN design UX on top of stable API

**Requirements for "Stable"**:
- ‚úÖ Zero UI dependencies (works in Node.js)
- ‚úÖ Dependency injection (testable)
- ‚úÖ Unit tests passing (100% coverage)
- ‚úÖ Manual testing via API Playground
- ‚úÖ Error scenarios handled
- ‚úÖ Performance acceptable

**Why This Order Matters**:
- API bugs discovered during UX work = rework both layers
- Stable API first = UX can focus on experience, not fixing API
- Testing catches issues before UX investment
- API Playground validates before committing to UX design

**Example Workflow**:
```javascript
// 1. Implement API
class FeatureAPI {
    async getFeatures() { /* logic */ }
}

// 2. Test API (100% coverage)
test('getFeatures returns list', async () => { ... });

// 3. Test in API Playground
// Verify: GET /api/features works perfectly

// 4. ONLY THEN: Design UX
const oList = new sap.m.List({ ... });
```

**Never**: Start with UX mockup and "figure out API later"

---

### 5. Modular Architecture ‚≠ê PROJECT STANDARD

**CRITICAL ENFORCEMENT**: Run quality gate before ANY module is considered complete:
```bash
python core/quality/module_quality_gate.py [module_name]
# MUST exit 0 (PASSED) before module goes live
```

**Complete Module Structure** (ALL components REQUIRED):
```
modules/[module-name]/
‚îú‚îÄ‚îÄ module.json          # MUST include backend.blueprint if backend/ exists
‚îú‚îÄ‚îÄ backend/             # Python services
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py      # MUST export blueprint
‚îÇ   ‚îú‚îÄ‚îÄ api.py           # MUST define Blueprint()
‚îÇ   ‚îî‚îÄ‚îÄ service.py
‚îú‚îÄ‚îÄ tests/               # Module tests
‚îú‚îÄ‚îÄ docs/                # Module docs (optional but recommended)
‚îî‚îÄ‚îÄ README.md            # Module documentation (recommended)
```

**module.json CONTRACT** (If module has backend/):
```json
{
  "name": "module_name",
  "version": "1.0.0",
  "description": "...",
  "enabled": true,
  "category": "...",
  "structure": {
    "backend": "backend/"
  },
  "backend": {
    "blueprint": "module_api",           // ‚Üê REQUIRED
    "module_path": "modules.module_name.backend"  // ‚Üê REQUIRED
  }
}
```

**backend/__init__.py CONTRACT**:
```python
from .api import module_api  # Must export blueprint

__all__ = ['module_api']
```

**Validation Rules**:
- ‚úÖ Module Quality Gate MUST pass (exit 0)
- ‚úÖ No DI violations (.connection, .service, .db_path access)
- ‚úÖ No direct module imports (loose coupling)
- ‚úÖ Blueprint self-registers via module.json
- ‚úÖ Uses interfaces from core.interfaces

**Benefits**: Self-contained, plug-and-play, feature-toggleable, ENFORCED quality

**Reference**: `core/quality/README.md`, `core/README.md`, [[Modular Architecture]]

**NEVER create a module without running the quality gate** - it enforces ALL these principles automatically.

---

### 5. API-First Development ‚≠ê MANDATORY

**Principle**: Implement business logic as APIs before UI

**Requirements**:
- ‚úÖ Zero UI dependencies
- ‚úÖ Dependency injection
- ‚úÖ Async/Promise-based
- ‚úÖ Works in Node.js, browser, CLI
- ‚úÖ JSDoc documented

**Example**:
```javascript
export class FeatureAPI {
    constructor(storageService) {
        this.storage = storageService; // Injected!
    }
    
    async getData() {
        return await this.storage.load(); // Pure logic
    }
}
```

---

### 6. Comprehensive Testing ‚≠ê MANDATORY - ALWAYS INCLUDE

**CRITICAL RULE**: Testing is NOT optional. ALWAYS create tests BEFORE claiming completion.

**When to Write Tests** (ALL these scenarios):
- ‚úÖ Creating new API endpoints ‚Üí Write unit tests
- ‚úÖ Creating new features ‚Üí Write integration tests
- ‚úÖ Refactoring code ‚Üí Write regression tests
- ‚úÖ Fixing bugs ‚Üí Write test that reproduces bug first
- ‚úÖ BEFORE attempting completion ‚Üí Tests MUST exist

**Standards**:
- ‚úÖ 100% API method coverage (MANDATORY)
- ‚úÖ Tests run in Node.js (not browser)
- ‚úÖ Tests run in < 5 seconds
- ‚úÖ Mock dependencies for isolation
- ‚úÖ Test success AND error scenarios
- ‚úÖ Integration tests for workflows

**Test Structure** (REQUIRED for all features):
```
modules/[module]/tests/
‚îú‚îÄ‚îÄ test_[feature]_unit.py       # Unit tests (API methods)
‚îú‚îÄ‚îÄ test_[feature]_integration.py # Integration tests (workflows)
‚îî‚îÄ‚îÄ README.md                      # Test documentation
```

**Python Testing**:
- ‚úÖ Use pytest
- ‚úÖ Mock external dependencies
- ‚úÖ Test both success and failure paths
- ‚úÖ Descriptive test names (test_[what]_[when]_[expected])

**Example Workflow** (MANDATORY):
1. Write feature code
2. Write unit tests (100% coverage)
3. Write integration tests (key workflows)
4. Run tests: `pytest path/to/tests -v`
5. ONLY THEN: Commit code + tests together
6. NEVER commit code without tests

**UI Testing**:
- ‚úÖ Use jsdom for browser simulation
- ‚úÖ Catch SAP UI5 API misuse
- ‚úÖ Tests run in < 10 seconds

**Enforcement** (AI MUST DO THIS):
Before using `attempt_completion`, AI must ask in `<thinking>`:
1. ‚ùì Have I written unit tests for all new code?
2. ‚ùì Have I written integration tests for workflows?
3. ‚ùì Have I run the tests to verify they pass?
4. ‚ùì Are tests committed with the code?

**If ANY answer is NO**: Write the missing tests FIRST.

**User Expectation**: 
> "Don't make me ask for tests. Include them automatically."
> "Tests are part of the deliverable, not an afterthought."

**Reference**: [[Testing Standards]] in knowledge vault

---

### 6.1 Browser Testing: LAST RESORT ONLY ‚ö†Ô∏è CRITICAL

**RULE**: Use `browser_action` tool ONLY as absolute last resort when all other methods fail

**WHY Browser Testing Should Be Avoided**:
- ‚ùå Inefficient: 60-300 seconds vs 1-5 seconds for API tests
- ‚ùå Unreliable: Can crash system, resource-intensive
- ‚ùå Not automatable: Cannot run in CI/CD pipelines
- ‚ùå Wastes time: Manual clicking vs automated validation
- ‚ùå System stability risk: User's primary concern

**CORRECT Testing Methods (Try These FIRST)**:
1. **Direct API Testing**: Use `test_api_endpoints.py` (5 seconds) ‚≠ê PREFERRED
2. **Unit Tests**: Use pytest with existing test files (automated) ‚≠ê PREFERRED
3. **CLI Validation**: Use `check_*.py` scripts (instant) ‚≠ê PREFERRED
4. **Integration Tests**: Use `test_api_direct.py` pattern (1-5 seconds) ‚≠ê PREFERRED
5. **Manual curl/requests**: Quick API endpoint verification (1-2 seconds)

**Time Comparison**:
- ‚úÖ API test: 1-5 seconds, automated, reliable, safe
- ‚ùå Browser test: 60-300 seconds, manual, crash-prone, risky

**When Browser Testing May Be Considered**:
- ‚úÖ ONLY after ALL other testing methods have been exhausted
- ‚úÖ ONLY when absolutely nothing else works
- ‚úÖ ONLY for final UX validation AFTER API is stable

**Enforcement - Before Using browser_action**:
1. ‚ùì Have I tried `test_api_endpoints.py`?
2. ‚ùì Have I tried pytest on the module tests?
3. ‚ùì Have I tried curl or direct API calls?
4. ‚ùì Is this truly the LAST resort?

**If ANY answer is NO**: Use the appropriate testing method instead.

**User Constraint**: System stability is critical - browser testing causes crashes and wastes time.

---

### 7. SAP Fiori Compliance ‚≠ê MANDATORY

**Critical Rules**:
1. ‚úÖ **Use standard controls first** (InputListItem, not CustomListItem)
2. ‚úÖ **No CSS hacks** (no !important, no custom padding overrides)
3. ‚úÖ **Pure JavaScript preferred** (easier debugging than XML)
4. ‚úÖ **Start simple** (title + one control ‚Üí incremental complexity)

**Quick Check**:
- Which Fiori control for this purpose?
- Standard control available? Use it!
- Need CSS fix? Wrong control chosen!

**Reference**: [[SAP Fiori Design Standards]], [[InputListItem Control Decision]]

---

## üéØ PRIORITY 3: OPERATIONAL PRACTICES

### 8. Test Server Cleanup ‚ö†Ô∏è MANDATORY

**CRITICAL RULE**: Always kill test servers after debugging/testing sessions complete

**The Problem**:
- AI starts multiple parallel test server instances during debugging
- User tests on wrong/stale server, sees old code behavior
- Result: Wasted 30-90 minutes debugging phantom issues

**MANDATORY WORKFLOW**:

**After testing/debugging complete, BEFORE asking user to verify**:
```bash
# Kill all Python processes (test servers)
taskkill /F /IM python.exe  # Windows
pkill python                # Linux/Mac
```

**When to do this** (ALL these scenarios):
- ‚úÖ After extensive debugging sessions
- ‚úÖ After root cause analysis complete
- ‚úÖ After multiple test server restarts
- ‚úÖ Before asking user to verify results
- ‚úÖ When switching from testing to implementation

**Correct Workflow**:
```
Test ‚Üí Debug ‚Üí Fix ‚Üí CLEANUP SERVERS ‚Üê MANDATORY ‚Üí Ask user to verify
```

**Why This Matters**:
- Prevents user testing on wrong/stale server
- Avoids 30-90 minutes of phantom debugging
- Respects user's time and system resources
- Clean slate for next test session
- "Works for me but not for user" syndrome eliminated

**User Feedback**: 
> "We spent a lot of time until we noticed wrong server"

**AI Self-Check Before Completion**:
1. ‚ùì Did I start test servers during this session?
2. ‚ùì Have I killed all test servers?
3. ‚ùì Am I about to ask user to verify?

**If ANY answer needs action**: Kill servers FIRST with `taskkill /F /IM python.exe`

**Never Skip This**: Server cleanup is NOT optional, it's MANDATORY

---

### 9. Application Logging

**Purpose**: Primarily for AI assistant troubleshooting

**Log**:
- ‚úÖ API calls, database queries, errors
- ‚úÖ Performance metrics, business decisions
- ‚ùå Sensitive data, debug statements

**When issues occur**: Check logs FIRST via UI "Logs" button

---

### 9. Git Workflow & Auto-Archive ‚≠ê AUTOMATED

**AI Does**:
- ‚úÖ Stage: `git add <files>`

**AI Does NOT**:
- ‚ùå Commit (user decides when - prefers batch commits)
- ‚ùå Push to GitHub (user decides when)
- ‚ùå Pull changes
- ‚ùå Remote operations

**User Preference**: Batch commits over frequent commits (see section 1.2)

**Tagging**: User creates tags for major milestones

---

### 9.1 Tag-Triggered Auto-Archive ‚≠ê AUTOMATED WORKFLOW

**RULE**: When user says "git push with tag vX.X", AI AUTOMATICALLY archives PROJECT_TRACKER.md

**Workflow** (AI executes WITHOUT asking):

1. **Detect tag request**: User mentions "tag v2.2" or "git push with tag"

2. **Get last tag**: `git describe --tags --abbrev=0`

3. **Extract milestone work**:
   - All entries since last tag date
   - Copy to `docs/archive/TRACKER-v{version}-{date}.md`
   - Include: Full session details, commits, learnings

4. **Compress main file**:
   - Keep only: Quick Resume Context (200 lines)
   - Keep only: Current Status Summary
   - Keep only: Next Actions
   - Keep only: Roadmap
   - Remove: Completed work (now in archive)

5. **Update main file header**:
   ```markdown
   ## Archive
   - [v2.2 (Jan 26-30)](docs/archive/TRACKER-v2.2-2026-01-30.md)
   - [v2.1 (Jan 19-25)](docs/archive/TRACKER-v2.1-2026-01-25.md)
   ```

6. **Commit archive changes**:
   ```bash
   git add docs/archive/ PROJECT_TRACKER.md
   git commit -m "[Archive] Archive v{version} work to docs/archive/"
   ```

7. **Then proceed with user's tag/push**:
   ```bash
   git tag -a v{version} -m "{description}"
   git push origin main --tags
   ```

**Archive File Structure**:
```markdown
# PROJECT_TRACKER Archive - v2.2 (Jan 26-30, 2026)

**Archived**: {date}
**Tag**: v2.2
**Commits**: {first_commit}..{last_commit}

## Work Performed
[Complete chronological log from PROJECT_TRACKER.md]

## Git Activity
[All commits during this milestone]

## Key Achievements
[Summary of major deliverables]
```

**Benefits**:
- ‚úÖ Zero manual work from user
- ‚úÖ Main file stays lean (~500 lines)
- ‚úÖ Complete history preserved
- ‚úÖ Self-contained archives
- ‚úÖ Git tags align with archives

**User Never Needs to Say**: "Archive the tracker" or "Move old entries"
**AI Just Does It**: When user mentions tagging

**Knowledge Graph Entry**: Store this workflow so all future AI sessions follow it automatically

---

### 10. Project Tracker

**Update `PROJECT_TRACKER.md` after**:
- ‚úÖ Feature completion
- ‚úÖ Major refactoring
- ‚úÖ Bug fixes
- ‚úÖ Architecture changes

**Format**: Version entry with objectives, work performed, metrics, status

---

## üìã AI ASSISTANT CHECKLIST

**Before implementing features**:

1. ‚úÖ Check knowledge graph
2. ‚úÖ Check knowledge vault docs
3. ‚úÖ **ASK: Should I implement discussed architecture first?** ‚≠ê NEW
4. ‚úÖ Create compliance checklist (all 7 requirements)
5. ‚úÖ Estimate FULL time (tests + docs + tracker)
6. ‚úÖ Get user approval
7. ‚úÖ Follow ALL steps
8. ‚úÖ Self-audit before commit

**NO SHORTCUTS**: Implement fewer features correctly, not more features poorly

---

## üéì Quick Reference

### Fiori UI Development
- Standard controls first
- No CSS hacks
- Pure JavaScript
- Start simple, build incrementally
- Check: [[SAP Fiori Design Standards]]

### Testing
- 100% API coverage
- Node.js tests < 5s
- UI tests < 10s
- Check: [[Testing Standards]]

### Architecture
- Modular structure: `modules/[name]/backend/`
- Feature toggleable
- Self-documented
- Check: [[Modular Architecture]]

### Documentation
- Knowledge vault: `docs/knowledge/`
- Use [[wikilinks]]
- Update INDEX.md
- Check: `docs/knowledge/README.md`

### Git
- Clear commit messages
- AI commits, user pushes
- Tags for milestones only
- Update PROJECT_TRACKER.md

---

## üìö Key References

**Knowledge Vault** (‚≠ê Start here):
- `docs/knowledge/INDEX.md` - All documentation
- [[SAP Fiori Design Standards]]
- [[Testing Standards]]
- [[Modular Architecture]]
- [[HANA Connection Module]]

**Legacy Docs** (detailed references):
- Fiori: `docs/fiori/SAP_FIORI_DESIGN_GUIDELINES.md`
- HANA: `docs/hana-cloud/` (25+ guides)
- Planning: `docs/planning/` (architecture & features)

---

**Summary**: Follow Priority 1 (essential workflows), then Priority 2 (development standards), then Priority 3 (operational practices). Reference knowledge vault for detailed guidance.