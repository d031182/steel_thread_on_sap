{
  "fields": {
    "project": {
      "key": "JIRASAN"
    },
    "reporter": {
      "name": "D031182"
    },
    "summary": "BDC-FOS and AI Core Integration for Batch Inference in AI Workflows",
    "description": "h1. Objective\n\nEnable native integration between SAP Business Data Cloud (BDC) Feature Orchestration Service (FOS) and SAP AI Core to support batch inference in AI workflows.\n\nh1. Business Value\n\n* Enable AI-powered insights on enterprise data without infrastructure complexity\n* Reduce operational overhead for AI workflows\n* Support multiple Line of Business use cases (Working Capital Insights, Predictive Cash Collection, Spend Control Tower)\n* Enable batch AI inference at scale (up to 1TB data volumes)\n\nh1. Functional Requirements\n\nh2. AI Core Use Case Support\n\n* Support 3 types of AI Core use cases:\n** Training (ArgoCD Workflow)\n** Batch Inferencing (primary focus)\n*** API supports batch (Q1 '26)\n*** Model applied on DataFrame (ArgoCD workflow)\n** Real-time Inferencing/LLM\n\nh2. End-to-End Integration\n\n* Seamless data flow from BDC-FOS to AI Core for batch processing\n* Support for both initial data load and incremental data load\n* Automated credential management (HDLFS certificates, AI Core tokens)\n\nh1. Technical Requirements\n\nh2. Architecture\n\n* Multi-tenant support:\n** One AI Core resource group per FOS tenant\n** Cross-region support (AI Core creates one subaccount per region)\n\n* Security & Authentication:\n** Use BTP Credential Store for secure credential management\n** HDLFS certificate rotation (< 3-day lifetime)\n** AI Core token management\n** No sensitive data in logs\n\n* Data Access:\n** Support for HDLFS (Hadoop Distributed Lake File System) access\n** Use delta-rs library with Datafusion (not Delta Share)\n** Certificate-based authentication\n\n* Resource Management:\n** AI Core resource group creation during data product provisioning\n** Mapping FOS tenants to AI Core resource groups\n** Support for 50+ resource groups (extendable)\n\nh2. Performance\n\n* Data Volume Limits:\n** Support up to 1TB (Datafusion single-node limitation)\n** Future exploration: Datafusion on Spark (Comet)\n\n* Execution Time:\n** AI Core workflow startup: ~8 minutes\n** Prefer fire-and-forget approach for long-running jobs\n\n* Cost Optimization:\n** Minimize read/write operations to reduce operational costs\n** Resource group-based pricing model\n\nh2. Operational\n\n* Observability:\n** Utilize existing BDC and AI Core monitoring tools\n** Support for OpenTelemetry (OTEL) Collector integration\n** Cloud Logging Systems (CLS) integration\n** Argo workflow state monitoring\n\n* Quality Assurance:\n** Confirm end-to-end functionality with known datasets\n** Support for multiple LoB use cases:\n*** Working Capital Insights (time series)\n*** Predictive Cash Collection (classification)\n*** Spend Control Tower\n\nh1. Constraints & Assumptions\n\nh2. Technical Constraints\n\n* HDLFS certificates have short lifetime (< 3 days) requiring rotation\n* Single node limitation for Datafusion (< 1TB data)\n* AI Core workflow startup time: ~8 minutes\n* Resource group limit: 50 default (can be extended)\n\nh2. Assumptions\n\n* Docker registry secrets populated during tenant setup\n* Central git repository for all Argo Workflow Templates\n* BDC-FOS has access to AI Core API credentials\n* All AI Core credentials stored in BTP Credential Store\n* Security team validation for credential management approach\n\nh2. Known Limitations\n\n* Only supporting batch inferencing where models are fetched from object store\n* Training pipeline must store model/metrics in HDLFS\n* Data volume < 1TB per job\n* PySpark vs. Datafusion performance benchmarking needed\n\nh1. Implementation Approach\n\nh2. Short-term (Q1 2026): Custom Transformer\n\n* PySpark-based transformer calling AI Core APIs\n* Generic transformer for all LoB use cases\n* Polling mechanism for job completion\n* BTP Credential Store integration\n\nh2. Long-term (Q2 2026): Plugin Framework\n\n* BDC FOS Plugin Extensibility Framework\n* Better separation of concerns\n* OpenTelemetry integration\n* Enhanced observability\n\nh2. Alternative Approach (Under Evaluation)\n\nKubernetes Job instead of Spark Transformer:\n* Pros: Optimal resource consumption, data volume agnostic, faster\n* Cons: New approach, integration complexity, state management\n\nh1. Key Decisions Pending\n\n* Tenancy Model: One AI Core tenant per region or per FOS tenant?\n* Configuration Timing: Create AI Core configuration at provisioning or runtime?\n* Certificate Management: Who owns HDLFS cert rotation?\n* Resource Limits: How many resource groups can be supported per AI Core instance?\n* Pricing: How are tenants mapped to AI Core resource groups for billing?\n\nh1. Success Criteria\n\nh2. Must Have\n\n* Secure credential management (BTP Cred Store)\n* Multi-tenant support (resource group per tenant)\n* Automated certificate rotation\n* End-to-end data flow (BDC-FOS â†’ AI Core)\n* Support for 1+ LoB use case\n\nh2. Should Have\n\n* Generic transformer (reusable across LoBs)\n* Fire-and-forget execution pattern\n* Observability via existing tools\n* Performance within acceptable limits\n\nh2. Nice to Have\n\n* Plugin framework (Q2 2026)\n* Support for > 1TB data volumes\n* Real-time inferencing support\n\nh1. Deliverables\n\n* End-to-end working solution in PoC/dev landscapes\n* Performance testing (boundary conditions)\n* Architecture Decision Records (ADRs)\n* Deployment guides\n* Pricing metrics documentation\n\nh1. Timeline\n\n* Immediate (2 weeks, end of October 2025):\n** Extend PoC to include end-to-end flows\n** Deploy to PoC/dev landscapes\n** Collaborate with LoB team for validation\n\n* Short-term (Q1 2026):\n** Implement custom transformer solution\n** Performance testing\n** Security validation\n\n* Long-term (Q2 2026):\n** Plugin framework implementation\n** OpenTelemetry integration\n** Enhanced observability\n\nh1. Stakeholders\n\n* LoB Teams: Validate practical use cases\n* Security Team: Approve credential management approach\n* BDC Operations: Certificate rotation ownership\n* AI Core Team: Deployment support, resource group limits\n\nh1. References\n\n* Source Document: BDC-FOS & AICore Integration.docx (v0.1, Sept 16, 2025)\n* Requirements Analysis: docs/knowledge/requirements/BDC_AI_CORE_INTEGRATION_REQUIREMENTS.md\n* Related ADRs:\n** ADR - BDC-BAI Integration for Batch Inference\n** BDC-BAI Tenancy Mapping ADR\n** WG_AI_ADR (Training and Grounding Workflows)\n* GitHub Repositories:\n** bdc-fos/dp-metadata (Data product definitions)\n** bdc-fos/fos-workflows (Transformer code)\n** delta-rs (SAP fork for HDLFS support)\n** hanalytics-techoffice/aicore_cash_forecast (Reference implementation)",
    "issuetype": {
      "name": "Requirement"
    }
  }
}