# Gu Wu Workflow for AI Assistant

**Date**: 2026-02-15
**Version**: 1.0
**Purpose**: Define what AI does when user says "run guwu test"
**Related**: [[Gu Wu API Contract Testing Foundation]]

---

## When User Says "Run Gu Wu Test"

### AI Workflow (MANDATORY)

**Step 1: Run Tests**
```bash
pytest tests/ -v
```
- Get test results
- Identify failures
- Note which tests pass/fail

**Step 2: Analyze Failures**
For each failing test, AI will:
1. âœ… Read the test file
2. âœ… Read the implementation file
3. âœ… Identify the root cause (e.g., signature mismatch, missing dependency)
4. âœ… Propose specific fix with code example
5. âœ… Explain WHY the fix works

**Step 3: Present Actionable Recommendations**
```
ğŸ¯ ISSUE #1: ConversationService signature mismatch
Test: test_conversation_service.py::test_create_conversation_via_service
Root Cause: Test passes repository=repo, but ConversationService.__init__() doesn't accept it
Fix: Check service __init__ signature and update either:
  Option A: Service to accept repository parameter
  Option B: Test to use correct instantiation
Confidence: HIGH
Impact: 2 tests affected
```

**Step 4: Offer to Fix**
Ask user: "Should I implement the fix?"

---

## What AI WILL Do (Automatic)

âœ… **Run pytest** - See results
âœ… **Analyze failures** - Read code, identify root cause
âœ… **Recommend fixes** - Specific, actionable, with code examples
âœ… **Explain WHY** - Root cause analysis
âœ… **Prioritize** - Impact assessment (how many tests affected)
âœ… **Offer to fix** - Ask if user wants AI to implement

---

## What AI WON'T Do (Unless User Requests)

âŒ **Automatic fixing** - Always ask first
âŒ **Gu Wu Intelligence engines** - Currently archived (tests/guwu/ deleted)
âŒ **ML predictions** - Intelligence layer not in current test structure
âŒ **Dashboard/metrics** - Phase 7 features archived

---

## Current Gu Wu Capabilities (v4.54)

### âœ… AVAILABLE (API Contract Testing Foundation)
1. **Run tests**: `pytest tests/ -v`
2. **API contract validation**: Tests via requests (< 1s)
3. **Manual analysis**: AI reads code and provides fix recommendations
4. **Test structure**: API-focused, refactoring-friendly
5. **Speed**: 60-300x faster than browser testing

### â³ ARCHIVED (Gu Wu Intelligence Engines)
1. **Recommendations Engine** - 8 types of insights (in archive)
2. **Dashboard Engine** - Visual health metrics (in archive)
3. **Predictive Engine** - ML failure forecasting (in archive)
4. **Auto-fix Generator** - Instant fix suggestions (in archive)
5. **Test Gap Analyzer** - Coverage gap detection (in archive)

**Location**: `archive/tests_backup_2026_02_15/guwu/`

**Can be restored** if user needs advanced intelligence features.

---

## Example Workflow

### User Says: "Run guwu test"

**AI Does**:
```bash
# 1. Run tests
pytest tests/ -v

# 2. Analyze results (AI reads code)
# Found: 2 failures in ConversationService tests

# 3. Present recommendations
```

**AI Response**:
```
Test Results: 15 passing âœ…, 2 failing âŒ

ğŸ¯ FAILURE ANALYSIS:

Issue #1: test_create_conversation_via_service
â”œâ”€ Error: TypeError: ConversationService.__init__() got unexpected keyword argument 'repository'
â”œâ”€ Root Cause: Test instantiates with repository=repo, but service doesn't accept it
â”œâ”€ Location: tests/test_ai_assistant_backend.py:158
â”œâ”€ Fix Options:
â”‚  A) Update service __init__ to accept repository parameter (DI pattern)
â”‚  B) Update test to use correct instantiation (check service code)
â””â”€ Recommendation: Check ConversationService.__init__() signature first

Issue #2: test_add_message_to_conversation
â”œâ”€ Same root cause as Issue #1
â””â”€ Fix: Same fix resolves both tests

Should I check the ConversationService code and propose specific fix?
```

**User**: "Yes"

**AI Does**:
```python
# Reads: modules/ai_assistant/backend/services/conversation_service.py
# Finds: def __init__(self): (no parameters)
# Proposes: Either add DI or update test
```

---

## Comparison: With vs Without Intelligence Engines

### WITHOUT Intelligence (Current v4.54)
```
User: "Run guwu test"
AI: Runs pytest â†’ Reads code â†’ Analyzes manually â†’ Recommends fix
Time: 30-60 seconds
Accuracy: 90% (AI analysis)
```

### WITH Intelligence (If Restored from Archive)
```
User: "Run guwu test"
AI: Runs pytest + intelligence â†’ ML analysis â†’ Prioritized recommendations
Time: 5-10 seconds
Accuracy: 95% (ML + patterns)
Features: Predictions, trends, auto-fix suggestions
```

---

## When to Restore Intelligence Engines

**Consider restoring if**:
- âœ… Test suite grows >100 tests
- âœ… Need failure prediction
- âœ… Want automated fix suggestions
- âœ… Need health trends over time
- âœ… Want coverage gap analysis

**Restore process**:
```bash
# Copy intelligence engines back
cp -r archive/tests_backup_2026_02_15/guwu/ tests/

# Verify imports work
python -m tools.guwu intelligence --help
```

---

## Key Point

**When you say "run guwu test"**, AI will:
1. âœ… Run pytest (see results)
2. âœ… Analyze failures (manual code reading)
3. âœ… Provide recommendations (specific fixes)
4. âœ… Explain root causes (WHY it failed)
5. âœ… Offer to implement fixes (if you approve)

This is **ALWAYS better than just pytest** because AI adds the analysis layer.

The archived Intelligence Engines would make this even faster/smarter, but manual analysis works well too.

---

## Summary

**Your Question**: "Will you apply all these tools when I ask to run guwu test?"

**Answer**: 
- âœ… **YES** - I run pytest + provide manual analysis + recommendations
- âš ï¸ **PARTIAL** - Intelligence engines archived (can restore if needed)
- âœ… **CORE** - API contract testing foundation is active and working

**Current workflow works great** for API contract testing. Intelligence engines are bonus features if you want ML-powered insights.